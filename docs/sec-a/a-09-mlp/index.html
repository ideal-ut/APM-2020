<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Multi-Layered Perceptron | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Multi-Layered Perceptron | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Jason Petri, Jordan Pflum, Immanuel Ponminissery. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Jason Petri, Jordan Pflum, Immanuel Ponminissery. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-09-mlp"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-09-mlp"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.ee71a697.js" as="script">
<link rel="preload" href="/APM-2020/main.34e32ce5.js" as="script">
<link rel="preload" href="/APM-2020/1.433d51c7.js" as="script">
<link rel="preload" href="/APM-2020/2.8e74170b.js" as="script">
<link rel="preload" href="/APM-2020/38.a3af94e3.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.4a8df433.js" as="script">
<link rel="preload" href="/APM-2020/17896441.b66b5b0d.js" as="script">
<link rel="preload" href="/APM-2020/b7fff368.4c61eafe.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-16-loss">Misclassification Rate and Minimizing Expected Loss</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Multi-Layered Perceptron</h1></header><div class="markdown"><p>Authors: Jason Petri, Jordan Pflum, Immanuel Ponminissery. (<a target="_blank" href="/APM-2020/assets/files/a-09-mlp-6f0fa4e7b538d3660bba61dc22a84668.pdf">PDF</a>)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="overview-of-lecture"></a>Overview of Lecture<a aria-hidden="true" tabindex="-1" class="hash-link" href="#overview-of-lecture" title="Direct link to heading">#</a></h2><p>During lecture, we first went over the backpropogation algorithm. This discussion followed the discussion of the intuition behind backpropogation from the previous class. Next, there was a discussion about model complexity and adjusting comlexity for MLPs. We then discussed Deep Learning and went over Facebook&#x27;s Deepface paper on a high level. The class concluded with a discussion about choosing methods to solve problems and the issues of modeling inverse problems.</p><p>These notes will go over the following topics:</p><ul><li>Backpropogation algorithm</li><li>Complexity discussion of MLPs</li><li>Deep Learning</li><li>Choosing Machine Learning method</li><li>Issues with modeling inverse problems</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="backpropogation-algorithm"></a>Backpropogation Algorithm<a aria-hidden="true" tabindex="-1" class="hash-link" href="#backpropogation-algorithm" title="Direct link to heading">#</a></h2><p>The backpropogation algorithm is split into two parts: the forward pass and the backward pass. One of the most tricky aspects of explaining the algorithm is the notation itself. The discussion below is a general outline of the algorithm:</p><p>For each input vector x, the activation of the hidden units is calculated as shown:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>h</mi><mo>=</mo><mi>Ïƒ</mi><mo stretchy="false">(</mo><mi>x</mi><mo>Ã—</mo><msup><mi>w</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} h = \sigma(x\times w^1) \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">Ïƒ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>The activation of the output units is calculated as shown:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>a</mi><mo>=</mo><mi>Ïƒ</mi><mo stretchy="false">(</mo><mi>h</mi><mo>Ã—</mo><msup><mi>w</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} a = \sigma(h\times w^2) \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord mathnormal" style="margin-right:0.03588em">Ïƒ</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>The error at each output and the deriatives of the activations are then calculated:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>e</mi><mn>2</mn></msup><mo>=</mo><mi>a</mi><mo>âˆ’</mo><mi>t</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} e^2 = a-t \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>d</mi><mn>2</mn></msup><mo>=</mo><mi>a</mi><mo stretchy="false">(</mo><mn>1</mn><mo>âˆ’</mo><mi>a</mi><mo stretchy="false">)</mo><msup><mi>e</mi><mn>2</mn></msup><mi>Î·</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} d^2 = a(1-a) e^2 \eta \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord mathnormal">a</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal">e</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em">Î·</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>The error from the output is passed back to the hidden layer and the weights are adjusted as shown below:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>d</mi><mn>1</mn></msup><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><mn>1</mn><mo>âˆ’</mo><mi>h</mi><mo stretchy="false">)</mo><msup><mi>w</mi><mn>2</mn></msup><msup><mi>d</mi><mn>2</mn></msup><mi>Î·</mi></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} d^1 = h(1-h) w^2 d^2 \eta \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">h</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.03588em">Î·</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>w</mi><mn>2</mn></msup><mo>=</mo><msup><mi>w</mi><mn>2</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>h</mi><mo>Ã—</mo><msup><mi>d</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} w^2 = w^2 + (h \times d^2) \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msup><mi>w</mi><mn>1</mn></msup><mo>=</mo><msup><mi>w</mi><mn>1</mn></msup><mo>+</mo><mo stretchy="false">(</mo><mi>x</mi><mo>Ã—</mo><msup><mi>d</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} w^1 = w^1 + (x \times d^1) \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.524108em;vertical-align:-0.512054em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.012054em"><span style="top:-3.147946em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8641079999999999em"><span style="top:-3.113em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.512054em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>An illustration of neural network with the backpropogation algorithm is shown below:</p><p><img src="/APM-2020/assets/images/backprop-31e1cb589b9bf6523c61a872e06d05a6.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="complexity-discussion-of-mlps"></a>Complexity discussion of MLPs<a aria-hidden="true" tabindex="-1" class="hash-link" href="#complexity-discussion-of-mlps" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="model-complexity-for-mlp"></a>Model Complexity for MLP<a aria-hidden="true" tabindex="-1" class="hash-link" href="#model-complexity-for-mlp" title="Direct link to heading">#</a></h3><p>The complexity of a neural network is dependent upon:</p><ul><li>number of hidden units</li><li>amount of training</li></ul><p>The above point is best explained using the image seen in lecture:
<img src="/APM-2020/assets/images/complexity-4321e7b9e1c574e09ebb9ee34ffd6f57.png"></p><p>Essentially, it is possible to achieve the fit in M = 3 using M = 10 but with fewer epochs.</p><p>Consider complex classification problem in the following image. Where the orange dots represent a target outcome of -1 and blue dots represent a target outcome of +1. If we must classify all of these dots, we will need a complex model!</p><p><img src="/APM-2020/assets/images/jason_1-962c4a51d7c1918a52739ee28faac0d9.png"></p><p>As we have stated earlier, model complexity is a result of hidden features and the amount of training. Consider the following model on this classification problem.</p><p><img src="/APM-2020/assets/images/jason_2-e7462b96325adfc2a88021c798dc0e5d.png"></p><p>We can view the input nodes on the left, the weights to the output. Notice how there are no hidden layers! To interpret the output of the trained model, look at the background color.</p><p>After 300 epochs, the model obtained a test loss of 0.592. Above, the background colors do not correspond well with the training data in that spiral. We have not achieved an appropriate level of complexity from hidden layers. However, consider the same model with less training. The model is significantly less complex. This is the output of the model after only 100 epochs of training.</p><p><img src="/APM-2020/assets/images/jason_3-43c33aee04b14b963b0bc9a9121e3081.png"></p><p>Not very complex and relatively similar in complexity to the model after 300 epochs of training.</p><p>Consider this model with one hidden layer with six nodes.</p><p><img src="/APM-2020/assets/images/jason_4-f4a978dfdba7809ba4b58ef6637607f9.png"></p><p>This model is considerably more complex as the model has been able to fit the data after training for 200 epochs. The test loss is now 0.001 and we have achieved a more complex model.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="choosing-machine-learning-method"></a>Choosing Machine Learning Method<a aria-hidden="true" tabindex="-1" class="hash-link" href="#choosing-machine-learning-method" title="Direct link to heading">#</a></h2><p>When choosing a machine learning method, it is important to rember that every model&#x27;s benifits always comes with an associated cost (ie there&#x27;s no such thing as a free lunch). While there is no single method for choosing which machine learning algorithm to employ, a good starting point is to ask yourself questions about the project. What am I trying to accomplish with the algorithm? What type of data am I feeding in? Is the data noisy? Do I care about interpreblity at the end, or will I be satisfied with a high accurarcy model? Questions such as these can help narrow down which algorithm to utilize.</p><p><img src="/APM-2020/assets/images/jordan_1-8b63ee756288f37ffd3b77880a2b0da0.png"></p><p>Once you have answer a few of these questions, you can begin to move onto selecting a few models to test out. It is always useful to reference documentation that lists the strengths and weakness of every algorith (see Sources). Say we have have a realtivley small dataset (50000,20) and would like to that we know to be very noisy. Additionally, the data is medical data, with every column being numeric except for the prediction column, which is binary. Given this information we could conclude that logistic regression would be a prime canidate to model the data. We can conclude this because a. We require a prediction model, b. Logistic regression is robust to noise, c. The size of the data set would not be computationally expensive for logistic regression.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="issues-with-modeling-inverse-problems"></a>Issues with Modeling Inverse Problems<a aria-hidden="true" tabindex="-1" class="hash-link" href="#issues-with-modeling-inverse-problems" title="Direct link to heading">#</a></h2><p>The importance of checking whether the erros are normally distributed is clearly seen when attempting to model Inverse Problems. Inverse Problems occur when attempting to calculate factors from a set of obeservations produced them. An example of such a problem is attempting to determine speed and path of a planet through its distance from earth. Through previous experiments, scientists knew that to determine the speed <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span> from measured positions <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">g(t)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span></span></span></span></span> you could employ the following formula:
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mo>=</mo><msub><mi>f</mi><mn>0</mn></msub><mo>+</mo><msubsup><mo>âˆ«</mo><mn>0</mn><mn>0</mn></msubsup><mi>f</mi><mo stretchy="false">(</mo><mi>Ï„</mi><mo stretchy="false">)</mo><mi>d</mi><mi>Ï„</mi></mrow><annotation encoding="application/x-tex">g(t) = f_{0} + \int^{0}_{0} f(\tau)d\tau</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">g</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.10764em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.3648280000000002em;vertical-align:-0.35582em"></span><span class="mop"><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0005599999999999772em">âˆ«</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.0090080000000001em"><span style="top:-2.34418em;margin-left:-0.19445em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span><span style="top:-3.2579000000000002em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">0</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.35582em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.10764em">f</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.1132em">Ï„</span><span class="mclose">)</span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:0.1132em">Ï„</span></span></span></span></span></p><p>However, it becomes an inverse problem as the speed of the planet directly impacts the distance from earth. A more general case can be seen in the following image:</p><p><img src="/APM-2020/assets/images/jordan_2-7613250f7ee020c760a19eb55742347c.png"></p><p>Here you can see that the measure output of the physical system does not so much cause t and t causes/effecct x. Such problem can easily be identified by looking at the distribution of the error terms. If the error terms are grouped on either side of the mean, it can be a good indication that the data is suffering from an inverse problem and should be modeling with a joint pdf or piecewise models.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="sources"></a>Sources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#sources" title="Direct link to heading">#</a></h2><ul><li><a href="https://www.researchgate.net/figure/Illustration-of-an-ANN-structure-with-backpropagation-algorithm_fig2_323328341" target="_blank" rel="noopener noreferrer">https://www.researchgate.net/figure/Illustration-of-an-ANN-structure-with-backpropagation-algorithm_fig2_323328341</a></li><li><a href="http://www.ai.mit.edu/courses/6.034b/backprops.pdf" target="_blank" rel="noopener noreferrer">http://www.ai.mit.edu/courses/6.034b/backprops.pdf</a></li><li><a href="https://ieor.berkeley.edu/wp-content/uploads/2019/03/IJCNN2001.pdf" target="_blank" rel="noopener noreferrer">https://ieor.berkeley.edu/wp-content/uploads/2019/03/IJCNN2001.pdf</a></li><li><a href="https://playground.tensorflow.org/" target="_blank" rel="noopener noreferrer">https://playground.tensorflow.org/</a></li><li><a href="https://elitedatascience.com/machine-learning-algorithms" target="_blank" rel="noopener noreferrer">https://elitedatascience.com/machine-learning-algorithms</a></li></ul></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-22T00:55:23.000Z" class="docLastUpdatedAt_217_">10/21/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-08-nn"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Neural Networks</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-10-dp"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Data Preprocessing Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview-of-lecture" class="table-of-contents__link">Overview of Lecture</a></li><li><a href="#backpropogation-algorithm" class="table-of-contents__link">Backpropogation Algorithm</a></li><li><a href="#complexity-discussion-of-mlps" class="table-of-contents__link">Complexity discussion of MLPs</a><ul><li><a href="#model-complexity-for-mlp" class="table-of-contents__link">Model Complexity for MLP</a></li></ul></li><li><a href="#choosing-machine-learning-method" class="table-of-contents__link">Choosing Machine Learning Method</a></li><li><a href="#issues-with-modeling-inverse-problems" class="table-of-contents__link">Issues with Modeling Inverse Problems</a></li><li><a href="#sources" class="table-of-contents__link">Sources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.ee71a697.js"></script>
<script src="/APM-2020/main.34e32ce5.js"></script>
<script src="/APM-2020/1.433d51c7.js"></script>
<script src="/APM-2020/2.8e74170b.js"></script>
<script src="/APM-2020/38.a3af94e3.js"></script>
<script src="/APM-2020/f976f453.4a8df433.js"></script>
<script src="/APM-2020/17896441.b66b5b0d.js"></script>
<script src="/APM-2020/b7fff368.4c61eafe.js"></script>
</body>
</html>