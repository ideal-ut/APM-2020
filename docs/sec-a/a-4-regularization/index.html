<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Regularization | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Regularization | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Benjamin Deutsch, Emah Eguche, Tairan Deng"><meta data-react-helmet="true" property="og:description" content="Authors: Benjamin Deutsch, Emah Eguche, Tairan Deng"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-4-regularization"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-4-regularization"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.4c657f01.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.7ee4ba75.js" as="script">
<link rel="preload" href="/APM-2020/main.8c9a4aa5.js" as="script">
<link rel="preload" href="/APM-2020/1.1df992a3.js" as="script">
<link rel="preload" href="/APM-2020/2.fdb9e081.js" as="script">
<link rel="preload" href="/APM-2020/22.3154d341.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.7147778a.js" as="script">
<link rel="preload" href="/APM-2020/17896441.3c98e833.js" as="script">
<link rel="preload" href="/APM-2020/c6fc3df4.f3486bb8.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-3-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-a/a-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-5-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-6-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-7-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-3-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-5-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-6-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-7-sgd">Stochastic Gradient Decent</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Regularization</h1></header><div class="markdown"><p>Authors: Benjamin Deutsch, Emah Eguche, Tairan Deng </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="lecture-content"></a>Lecture Content<a aria-hidden="true" tabindex="-1" class="hash-link" href="#lecture-content" title="Direct link to heading">#</a></h2><p>Sept 16th lecture was mainly about the following topics;</p><ol><li>Regularization and overfitting issues</li><li>Parameter/component evaluation</li><li>Bias-Variance dilemma</li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="regularization--overfitting"></a>Regularization &amp; Overfitting<a aria-hidden="true" tabindex="-1" class="hash-link" href="#regularization--overfitting" title="Direct link to heading">#</a></h3><p>Polynomial models of higher dimension are not always appropriate when dealing with predictive modeling. For example: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>X</mi><mi>M</mi></msup></mrow><annotation encoding="application/x-tex">X^{M}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span></span></span></span></span></span></span></span></span> given, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">M=3</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em"></span><span class="mord">3</span></span></span></span></span> and <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>9</mn></mrow><annotation encoding="application/x-tex">M=9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em"></span><span class="mord">9</span></span></span></span></span>. When <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mn>9</mn></mrow><annotation encoding="application/x-tex">M=9</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em"></span><span class="mord">9</span></span></span></span></span>, the model is complicated and is said to overfit, meaning the model exactly replicates of all training data. This is not useful because it is a model tailor made for that data set and will have limited if not any predictive power with respect to sampling changes in the data.  Even the M=3 model does not have execellent prediction on the training data, still it can be said is a better option because it  it offers more flexbility for alternative sample data set and maintaining some predictive powers. Underfitting occurs with ridge function such as M=1 where the model output can has no motion to accomodate the datas inner points. </p><p>See Below:</p><p><img alt="alt text" src="/APM-2020/assets/images/one-9d51ba74f316b803ed217c16eb93fc42.png" title="Overfitting"></p><p>$$ COST = MSE + \lambda * Penalty(f) $$</p><p>Generally speaking, lower order polynomial works better on lower data sparsity and higher polynomial model are good for high sparsity of data. In order to facilitate the alternatives to the general rule, we introduce a penalty which reacts to the weights of the particular data, in a process called regularizatrion. This &quot;Cost&quot; as referred to above is the result of the MSE given by the dataset, the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal">Î»</span></span></span></span></span> or &quot;tradeoff&quot; multiplier and the &quot;Penalty&quot; function which is empirically chosen.<br>
It should be noted that weight 0 is not included in the penalty calculation since Penalty(f) is sum of weight from 1 to M. The Mean Squared Error is the mean squared value of residuals from the models data fitting regression plot. Again as a reminder the purpose of this standardization is to create a model with certain fit and smaller weights are normally preferred.</p><p>As a rule of thumb higher data complexity of a data set is usually referred to data sets with higher collinearity, meaning, componet values will be indicators for other componet values. In english column values will rely and have results because of other columns values. This is an issue as it amplifies the effects of these componets in the models practice. And so, given this higher collinearity will usually result in higher penalties. The issue here is mitigating that effect. So enters two types of regression modelings; Ridge,</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>âˆ‘</mo><mi>M</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></msubsup><msubsup><mi>w</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sum_{M}^{i=1} w_{i}^{2}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.264274em;vertical-align:-0.29971000000000003em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">âˆ‘</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.964564em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em"><span style="top:-2.441336em;margin-left:-0.02691em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.258664em"><span></span></span></span></span></span></span></span></span></span></span></p><p>And Lasso,</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mo>âˆ‘</mo><mi>M</mi><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow></msubsup><mrow><mo fence="true">âˆ¥</mo><msub><mi>w</mi><mi>i</mi></msub><mo fence="true">âˆ¥</mo></mrow></mrow><annotation encoding="application/x-tex">\sum_{M}^{i=1} \left \|w_{i} \right \|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.264274em;vertical-align:-0.29971000000000003em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">âˆ‘</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.964564em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em">âˆ¥</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em">âˆ¥</span></span></span></span></span></span></p><p>Each of these has machinizations built in for handling different data set types. </p><p>Lasso penalty is absolute value of $\w, weights are linear (&quot;V&quot; shaped) given the componet volume of the set. Giving this the ability to drop componets to zero. While Ridge penalty is sum squared of weights, which increases exponentially as the component weights increases  &#x27;U&#x27; shape curve. Unbiasing the small weights aganist the larger weighted classes.</p><p>Without regularization, the weights would be very high value and create a high cost.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mi>N</mi><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow></msubsup><msup><mrow><mo fence="true">{</mo><msup><mi>w</mi><mi>T</mi></msup><mi>Ï•</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>âˆ’</mo><msup><mi>t</mi><mi>n</mi></msup><mo fence="true">}</mo></mrow><mn>2</mn></msup><mo>+</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><msup><mrow><mo fence="true">âˆ¥</mo><mi>w</mi><mo fence="true">âˆ¥</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">E(w) = \sum_{N}^{n=1} \left \{ w^{T} \phi(x_{n}) - t^{n} \right \}^{2} + \frac{\lambda}{2}\left \| w \right \|^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.404018em;vertical-align:-0.35001em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">âˆ‘</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.954008em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size1">{</span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span></span><span class="mord mathnormal">Ï•</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size1">}</span></span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:1.054008em"><span style="top:-3.3029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.299008em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Î»</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em">âˆ¥</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose delimcenter" style="top:0em">âˆ¥</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em"><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p>Therefore, we prefer to use Ridge for higher complexity models and Lasso is a better soltuion for low complexity models.</p><p>In the graph of Ridge and Lasso lambda comparison, we can see differences in the models on optimizing parameters. The degree of freedom is inversely related to lambda. After fitting two models with cross validation and finding out the best lambda for prediction, we can see in the Lasso model the best fitting lambda excluded 3 parameters of the model. In practice if you have a lot of parameters, use Lasso to figure out which parameters are noisy and drop them. Use Ridge to manage a colinarity problem.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="quiz-review"></a>Quiz review<a aria-hidden="true" tabindex="-1" class="hash-link" href="#quiz-review" title="Direct link to heading">#</a></h3><p>Trick question: A dataset is devided half &amp; half for set A and set B. Run OLS model on set A and with good luck we found the optimal parameter vector, i.e. Beta. Then if we use the optimal parameter model for dataset A and B, what MSE would we expect? The right answer is MSE of Dataset B will be similar to that of Dataset A. The error is coming from both A and B, then the error is evenly distributed between A and B. Under optimal parameter, the residuals are from the model, not from lambda or penalty. In the model fitting process, A and B shared same amount of errors and will return similar MSE.</p><p>MLR model question.</p><ol><li><p>Independent variables are measured error-free, so there is no noise associated with them.</p></li><li><p>Independent variables are independent from each other. </p></li></ol><p>Variables can still have errors even they are independent from each other and noise will exist due to errors. Removing collinearity of variables will not reduct errors and noise.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bias-variance-dilemma"></a>Bias-Variance Dilemma<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bias-variance-dilemma" title="Direct link to heading">#</a></h3><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>L</mi><mo stretchy="false">]</mo><mo>=</mo><mo>âˆ«</mo><msup><mrow><mo fence="true">{</mo><mi>y</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>âˆ’</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo fence="true">}</mo></mrow><mn>2</mn></msup><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mi>d</mi><mi>x</mi><mo>+</mo><mo>âˆ«</mo><mo>âˆ«</mo><msup><mrow><mo fence="true">{</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>âˆ’</mo><mi>t</mi><mo fence="true">}</mo></mrow><mn>2</mn></msup><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>t</mi><mo stretchy="false">)</mo><mi>d</mi><mi>x</mi><mi>d</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">E[L] = \int \left \{ y(x) - h(x) \right \}^2 p(x)dx + \int \int \left \{ h(x)- t \right \}^2 p(x,t)dx dt</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mopen">[</span><span class="mord mathnormal">L</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.260128em;vertical-align:-0.30612em"></span><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0005599999999999772em">âˆ«</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em">{</span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose delimcenter" style="top:0em">}</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em"><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.260128em;vertical-align:-0.30612em"></span><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0005599999999999772em">âˆ«</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mop op-symbol small-op" style="margin-right:0.19445em;position:relative;top:-0.0005599999999999772em">âˆ«</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="minner"><span class="minner"><span class="mopen delimcenter" style="top:0em">{</span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">t</span><span class="mclose delimcenter" style="top:0em">}</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.954008em"><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mord mathnormal">d</span><span class="mord mathnormal">x</span><span class="mord mathnormal">d</span><span class="mord mathnormal">t</span></span></span></span></span></p><p>The error of a model has 2 components; controllable error depending on the Probability Density Function of x and the uncontrollable independent noise. In the senario mentioned in class, 3 models fit differently when X is in different ranges. For a certain range of X, a certain model will be the best fit. However when X is randomly drawn, different model fits will produce different level of errors. The purpose of the PDF, P(x), is to reduce the error incurred due to randomness of x value in the model, which is represented by y(x)-h(x).</p><p>For the independent noise part, it can not be optimized because of it is cause by x and t, as zero mean noise.In the ideal case, we want to totally eliminate the error caused by probability density of x and the only residual error would be the zero mean noise.</p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">[</mo><mi>t</mi><mi mathvariant="normal">/</mi><mi>x</mi><mo stretchy="false">]</mo><mo>=</mo><mi>h</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">E[t/x]=h(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mopen">[</span><span class="mord mathnormal">t</span><span class="mord">/</span><span class="mord mathnormal">x</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">h</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="addtional-resources"></a>Addtional Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#addtional-resources" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="regulation-and-overfitting"></a>Regulation and Overfitting<a aria-hidden="true" tabindex="-1" class="hash-link" href="#regulation-and-overfitting" title="Direct link to heading">#</a></h3><p>More information on regularization and overfitting in this article: <a href="https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76" target="_blank" rel="noopener noreferrer">https://medium.com/greyatom/what-is-underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6803a989c76</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bias-variance-dilemma-1"></a>Bias-Variance Dilemma<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bias-variance-dilemma-1" title="Direct link to heading">#</a></h3><p>More information on the bias-variance tradeoff in this article: <a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229</a></p><p>More information on the bias-variance tradeoff in this video: <a href="https://www.youtube.com/watch?v=EuBBz3bI-aA&amp;feature=emb_title" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=EuBBz3bI-aA&amp;feature=emb_title</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-09-25T04:27:42.000Z" class="docLastUpdatedAt_217_">9/24/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-3-parametric-models"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Parametric Models</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-5-bias-variance"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Bias and Variance Dilemma &amp; Shrinkage Methods Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#lecture-content" class="table-of-contents__link">Lecture Content</a><ul><li><a href="#regularization--overfitting" class="table-of-contents__link">Regularization &amp; Overfitting</a></li><li><a href="#quiz-review" class="table-of-contents__link">Quiz review</a></li><li><a href="#bias-variance-dilemma" class="table-of-contents__link">Bias-Variance Dilemma</a></li></ul></li><li><a href="#addtional-resources" class="table-of-contents__link">Addtional Resources</a><ul><li><a href="#regulation-and-overfitting" class="table-of-contents__link">Regulation and Overfitting</a></li><li><a href="#bias-variance-dilemma-1" class="table-of-contents__link">Bias-Variance Dilemma</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.4c657f01.js"></script>
<script src="/APM-2020/runtime~main.7ee4ba75.js"></script>
<script src="/APM-2020/main.8c9a4aa5.js"></script>
<script src="/APM-2020/1.1df992a3.js"></script>
<script src="/APM-2020/2.fdb9e081.js"></script>
<script src="/APM-2020/22.3154d341.js"></script>
<script src="/APM-2020/f976f453.7147778a.js"></script>
<script src="/APM-2020/17896441.3c98e833.js"></script>
<script src="/APM-2020/c6fc3df4.f3486bb8.js"></script>
</body>
</html>