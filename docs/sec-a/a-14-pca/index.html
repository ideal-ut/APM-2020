<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Principal Components Analysis | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Principal Components Analysis | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors:  Dian Zhao, Letian Zhao, Yuhan Yin, Yiwei Zhou. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors:  Dian Zhao, Letian Zhao, Yuhan Yin, Yiwei Zhou. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-14-pca"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-14-pca"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.00f904be.js" as="script">
<link rel="preload" href="/APM-2020/main.34e32ce5.js" as="script">
<link rel="preload" href="/APM-2020/1.433d51c7.js" as="script">
<link rel="preload" href="/APM-2020/2.8e74170b.js" as="script">
<link rel="preload" href="/APM-2020/38.a3af94e3.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.4a8df433.js" as="script">
<link rel="preload" href="/APM-2020/17896441.b66b5b0d.js" as="script">
<link rel="preload" href="/APM-2020/2185ff4f.0866954a.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">üåú</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">üåû</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-16-loss">Misclassification Rate and Minimizing Expected Loss</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Principal Components Analysis</h1></header><div class="markdown"><p>Authors:  Dian Zhao, Letian Zhao, Yuhan Yin, Yiwei Zhou. (<a target="_blank" href="/APM-2020/assets/files/a-14-pca-a017807a582b1ce3556a197cc877dfb2.pdf">PDF</a>)</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="lecture-content"></a>Lecture Content<a aria-hidden="true" tabindex="-1" class="hash-link" href="#lecture-content" title="Direct link to heading">#</a></h3><ol><li>PCA and its principles</li><li>Linear Supervised Method</li><li>Non-Linear Embeddings for Visualization</li><li>PCA and t-SNE</li><li>Bonferroni‚Äôs Theorem</li></ol><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="pca-and-its-principles"></a>PCA and its principles:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#pca-and-its-principles" title="Direct link to heading">#</a></h2><ul><li>Principal Component Analysis (PCA) - pay attention to the spelling of ‚ÄúPrincipal‚Äù</li><li><strong>PCA</strong> is a standard technique for visualizing high dimensional data and for data pre-processing. PCA finds the best ‚Äúsubspace‚Äù based on eigen-decomposition of data covariance matrix to capture the most data variance possible while reducing the data dimensionality. This will also incur a loss that is represented by the perpendicular distance of each data point to the PCA directions during projection. </li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="sequential-optimality-of-pca"></a>Sequential optimality of PCA:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#sequential-optimality-of-pca" title="Direct link to heading">#</a></h3><ul><li>Every dimension PCA uses to capture the data variance is the optimal choice that minimizes the loss.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="scree-plot-visualization-of-how-efficient-the-eigenvalues-are-in-capturing-the-total-variance"></a>Scree Plot: visualization of how efficient the eigenvalues are in capturing the total variance:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#scree-plot-visualization-of-how-efficient-the-eigenvalues-are-in-capturing-the-total-variance" title="Direct link to heading">#</a></h3><p><img src="https://i.imgur.com/DlEzs6z.png" alt="Drawing">
<img src="https://i.imgur.com/dMIphn7.png" alt="Drawing"></p><p>Prof. Ghosh‚Äôs class examples on PCA and the corresponding scree plot</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="extensive-reading-on-pca-and-eigen-algebra"></a>Extensive reading on PCA and eigen algebra:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#extensive-reading-on-pca-and-eigen-algebra" title="Direct link to heading">#</a></h3><ul><li>The eigenvectors and eigenvalues of a covariance (or correlation) matrix is what we need to understand firstly before the PCA: Eigenvectors determine the directions of the new feature space and eigenvalues are the magnitude of data variance in the PCA dimensions. </li></ul><ul><li>The classic approach to PCA is to perform the eigendecomposition on the covariance matrix, in which each element is the covariance between the two corresponding features. Recap the following equation to calculate the covariance:</li></ul><p><img src="https://i.imgur.com/fZxyEZj.png"></p><ul><li>Covariance matrix:</li></ul><p><img src="https://i.imgur.com/aeTE0zy.png"></p><ul><li><p>After getting the eigenvectors and eigenvalues of the above matrix, we will inspect and drop the eigenvectors with the lowest eigenvalues because they incorporate the <strong>least</strong> information regarding data variance. </p></li><li><p>After sorting the eigenpairs, we should determine the number of principal components to be chosen for the new feature subspace. A useful measure is the <strong>explained variance</strong>, which can be calculated from the eigenvalues. It tells us how much information (variance) can be attributed to each of the principal components and is similar to the scree plot. We expect the first few eigenvalues capture about <strong>80%-90%</strong> explained variance.</p></li></ul><p><img src="https://i.imgur.com/NmJ9X7j.png">
Scree plot with the explained variance
Sample codes can be found through the website [1] in the reference list</p><ul><li>In our class example, we see a drastic decrease in eigenvalues as more principal components are selected. The first two principal component dimensions have dominated the analysis and captured 95% of the total variance. Intuitively, the number of dimensions we choose is much less needed than the total variance in percentage they could capture.</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="reducing-dimensionality"></a>Reducing dimensionality<a aria-hidden="true" tabindex="-1" class="hash-link" href="#reducing-dimensionality" title="Direct link to heading">#</a></h3><p>Naturally occurring data may have high dimensionality while its subspace is much lower. PCA is used to visualize these data by reducing the dimensionality. It rotates the original data space such that the axes of the new coordinate system point into the directions of
<em>highest variance</em> of the data. We can identify the two-dimensional plane that optimally describes the highest variance of the three-dimensional data below. </p><p><img src="https://i.imgur.com/EQ0Nahg.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="linear-supervised-method"></a>Linear Supervised Method<a aria-hidden="true" tabindex="-1" class="hash-link" href="#linear-supervised-method" title="Direct link to heading">#</a></h2><ul><li>PCA is unsupervised learning and is not the best solution for classification. </li><li>Ignoring colors, the PCA direction should be like the left picture. But when we consider color classification, the direction should be like the right picture. Red points are projected to a small range and blue points are also projected to a small range on the arrow.</li><li>A special Linear Supervised Method is Fisher‚Äôs Linear Discriminant (FLD) which finds the projection direction that best separates the two classes.</li><li>Multiple discriminant analysis (MDA) extends LDA to multiple classes.</li></ul><p><img src="https://i.imgur.com/hHBcI8n.png">
<img src="https://i.imgur.com/VjjgUV3.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="non-linear-embeddings-for-visualization"></a>Non-Linear Embeddings for Visualization<a aria-hidden="true" tabindex="-1" class="hash-link" href="#non-linear-embeddings-for-visualization" title="Direct link to heading">#</a></h2><ul><li><p>Manifold is a topological space with the property that each point has a neighborhood that is homeomorphic to the Euclidean space of dimension n. It captures the intrinsic dimensionality of data in a nonlinear fashion. </p></li><li><p>The earth is a three dimensional space, but its intrinsic dimensionality is two. In other words, a two dimensional manifold embedded in three dimension space.</p></li><li><p>A coil is intrinsically one dimension, but it&#x27;s embedded in three dimension space. </p></li><li><p>Application: capture handwritten digits using a two dimensional manifold.</p></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="pca-and-t-sne"></a>PCA and t-SNE<a aria-hidden="true" tabindex="-1" class="hash-link" href="#pca-and-t-sne" title="Direct link to heading">#</a></h2><ul><li>Project 784 (28*28 images) dimensions to 3 dimensions</li><li>Left picture is PCA, right one is t-distributed stochastic neighbor embedding (t-SNE)
<img src="https://i.imgur.com/vLeM9wH.png"><img src="https://i.imgur.com/uKkvFkV.png"></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bonferronis-theorem"></a>Bonferroni‚Äôs Theorem:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bonferronis-theorem" title="Direct link to heading">#</a></h2><p>If there are too many possible conclusions to draw, some will be true for purely statistical reasons, with no physical validity -&gt; correlation does not suggest causation„ÄÇ</p><p><img src="https://i.imgur.com/tcjjD3d.png">
<a href="http://www.tylervigen.com" target="_blank" rel="noopener noreferrer">www.tylervigen.com</a></p><p><img src="https://i.imgur.com/xo5gpxN.png">
<a href="https://www.youtube.com/watch?v=jbkSRLYSojo" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=jbkSRLYSojo</a></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="hans-rosling-ted-talk"></a>Hans Rosling Ted Talk<a aria-hidden="true" tabindex="-1" class="hash-link" href="#hans-rosling-ted-talk" title="Direct link to heading">#</a></h3><ul><li>The best stats you&#x27;ve ever seen | Hans Rosling: <a href="https://www.youtube.com/watch?v=hVimVzgtD6w" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=hVimVzgtD6w</a></li><li>How not to be ignorant about the world | Hans and Ola Rosling: <a href="https://www.youtube.com/watch?v=Sm5xF-UYgdg" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=Sm5xF-UYgdg</a></li><li>Hans Rosling: Global population growth, box by box:
<a href="https://www.youtube.com/watch?v=fTznEIZRkLg" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=fTznEIZRkLg</a></li><li>Religions and babies | Hans Rosling:
<a href="https://www.youtube.com/watch?v=ezVk1ahRF78" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=ezVk1ahRF78</a></li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="reference-and-further-readings"></a>Reference and further readings:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#reference-and-further-readings" title="Direct link to heading">#</a></h3><ol><li><a href="https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#1---eigendecomposition---computing-eigenvectors-and-eigenvalues" target="_blank" rel="noopener noreferrer">https://sebastianraschka.com/Articles/2015_pca_in_3_steps.html#1---eigendecomposition---computing-eigenvectors-and-eigenvalues</a></li><li><a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues" target="_blank" rel="noopener noreferrer">https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues</a></li></ol></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-31T08:10:29.000Z" class="docLastUpdatedAt_217_">10/31/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-13-dd-docs"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">¬´ Data-Driven Documents</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-01-gaussian-dist"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">The Gaussian Distribution ¬ª</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#lecture-content" class="table-of-contents__link">Lecture Content</a></li><li><a href="#pca-and-its-principles" class="table-of-contents__link">PCA and its principles:</a><ul><li><a href="#sequential-optimality-of-pca" class="table-of-contents__link">Sequential optimality of PCA:</a></li><li><a href="#scree-plot-visualization-of-how-efficient-the-eigenvalues-are-in-capturing-the-total-variance" class="table-of-contents__link">Scree Plot: visualization of how efficient the eigenvalues are in capturing the total variance:</a></li><li><a href="#extensive-reading-on-pca-and-eigen-algebra" class="table-of-contents__link">Extensive reading on PCA and eigen algebra:</a></li><li><a href="#reducing-dimensionality" class="table-of-contents__link">Reducing dimensionality</a></li></ul></li><li><a href="#linear-supervised-method" class="table-of-contents__link">Linear Supervised Method</a></li><li><a href="#non-linear-embeddings-for-visualization" class="table-of-contents__link">Non-Linear Embeddings for Visualization</a></li><li><a href="#pca-and-t-sne" class="table-of-contents__link">PCA and t-SNE</a></li><li><a href="#bonferronis-theorem" class="table-of-contents__link">Bonferroni‚Äôs Theorem:</a><ul><li><a href="#hans-rosling-ted-talk" class="table-of-contents__link">Hans Rosling Ted Talk</a></li><li><a href="#reference-and-further-readings" class="table-of-contents__link">Reference and further readings:</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright ¬© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.00f904be.js"></script>
<script src="/APM-2020/main.34e32ce5.js"></script>
<script src="/APM-2020/1.433d51c7.js"></script>
<script src="/APM-2020/2.8e74170b.js"></script>
<script src="/APM-2020/38.a3af94e3.js"></script>
<script src="/APM-2020/f976f453.4a8df433.js"></script>
<script src="/APM-2020/17896441.b66b5b0d.js"></script>
<script src="/APM-2020/2185ff4f.0866954a.js"></script>
</body>
</html>