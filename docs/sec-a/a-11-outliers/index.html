<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Handling Outliers | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Handling Outliers | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Vikram Seth, Steve Sohn, Xinmeng Song. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Vikram Seth, Steve Sohn, Xinmeng Song. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-11-outliers"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-a/a-11-outliers"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.4ee423d4.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.9ef0f007.js" as="script">
<link rel="preload" href="/APM-2020/main.a4e4d34c.js" as="script">
<link rel="preload" href="/APM-2020/1.50c3da69.js" as="script">
<link rel="preload" href="/APM-2020/2.84c4d38d.js" as="script">
<link rel="preload" href="/APM-2020/30.e9a85367.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.0e2787a5.js" as="script">
<link rel="preload" href="/APM-2020/17896441.8643754d.js" as="script">
<link rel="preload" href="/APM-2020/eb5e4ebc.19f9dbb1.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li></ul></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Handling Outliers</h1></header><div class="markdown"><p><strong>Authors:</strong> Vikram Seth, Steve Sohn, Xinmeng Song. (<a target="_blank" href="/APM-2020/assets/files/a-11-outliers-ec1eeca02d645c06893f3da83beee38f.pdf">PDF</a>)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="overview-of-lecture"></a>Overview of Lecture<a aria-hidden="true" tabindex="-1" class="hash-link" href="#overview-of-lecture" title="Direct link to heading">#</a></h2><p>The lecture first introduced several common data issues, and then explained that the solution is to use data pre-processing. Data pre-processing is a very frequently used process in daily data analysis. </p><p>Then it introduced how data cleaning is used and how to process missing values and outliers. Many algorithms used in feature selection and feature extraction are introduced in data reduction methods.</p><p>Our note will go over the following topics:</p><ul><li>Data Cleaning</li><li>Exploratory Data Analysis</li><li>Data Reduction</li><li>Data Visualization</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="data-cleaning"></a>Data cleaning<a aria-hidden="true" tabindex="-1" class="hash-link" href="#data-cleaning" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="dealing-with-missing-values"></a>Dealing with Missing Values<a aria-hidden="true" tabindex="-1" class="hash-link" href="#dealing-with-missing-values" title="Direct link to heading">#</a></h3><p>There are many reasons for missing data. For example, it may be because the respondent forgot to answer the question, or the questionnaire is too long and the questionnaire is too long, and the survey may not be completed. There may also be a problem with the recorded equipment or network error. There are several common ways to deal with missing values:</p><ul><li>Ignore the record or attribute(s)</li><li>Fill in missing values</li></ul><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="handling-outliers"></a>Handling Outliers<a aria-hidden="true" tabindex="-1" class="hash-link" href="#handling-outliers" title="Direct link to heading">#</a></h3><p>Outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set. For univariate detection of outliers, there is a simple and effective way: to use boxplot. Note that the outliers (the + markers in your plot) are simply points outside of the wide [(Q1-1.5 IQR), (Q3+1.5 IQR)] margin below.</p><p><img alt="title" src="/APM-2020/assets/images/boxplot-2caa08913ce0929c2c01ae1f82a2d75e.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="huber-loss"></a>Huber Loss<a aria-hidden="true" tabindex="-1" class="hash-link" href="#huber-loss" title="Direct link to heading">#</a></h3><p>Instead of identifying and eliminating outliers, we can use methods that are not as affected by outliers.</p><p>One of these methods is Huber Loss (1964).</p><p>Easiest way to explain Huber Loss would be that it is like a combination of Mean Squared Error and Mean Absolute Error, where it is similar to MSE (quadratic) for small numbers and similar to MAE (linear) for larger numbers. It is expressed by the following:</p><p><img src="https://i.imgur.com/M8Bf9Pd.jpg" alt="Huber Loss"></p><p>Refer to Additional resources for comparisons with other loss functions.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="python-outlier-detection-pyod"></a>Python Outlier Detection (PyOD)<a aria-hidden="true" tabindex="-1" class="hash-link" href="#python-outlier-detection-pyod" title="Direct link to heading">#</a></h3><p>PyOD includes over 20 outlier detection algorithms and is well-documented. </p><p>For a tutorial and a list of the algorithms visit <a href="https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/" target="_blank" rel="noopener noreferrer">here</a> or refer to other links in the additional resources section.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="exploratory-data-analysisdata-visualization"></a>Exploratory Data Analysis/Data Visualization<a aria-hidden="true" tabindex="-1" class="hash-link" href="#exploratory-data-analysisdata-visualization" title="Direct link to heading">#</a></h2><p>Data exploration and data visualization are generally inseparable. There are many graphical choices faced in visualization. There is a chart selection diagram created by Dr. Andrew Abela that should help you pick the right chart for your data type.</p><p><img alt="title" src="/APM-2020/assets/images/charts-9257d8931b855bdfa65e8c9408878e48.jpg"></p><p>First, we need to understand that data usually contains five related relationships: composition, comparison, trend, distribution, and connection.</p><p>The composition is mainly concerned with the percentage of each part of the whole. If the information you want to express includes: &quot;share&quot;, &quot;percentage&quot; and &quot;what percentage is expected to reach&quot;, then you can use a pie chart;</p><p>Comparison can show the order of thingsâ€”is it the same, or is one more or less than the other? &quot;Greater than&quot;, &quot;less than&quot; or &quot;approximately equal&quot; are all keywords in a comparative relative relationship. At this time, bar graphs are preferred;</p><p>Trend is the most common kind of time series relationship. It is concerned about how data changes over time. The weekly, monthly, and yearly trends are increasing, decreasing, fluctuating up and down or basically unchanged. At this time, it is better to use line graphs. The trend of the performance indicators over time;</p><p>Distribution is concerned with how many items are included in each value range. Typical information will include: &quot;concentration&quot;, &quot;frequency&quot; and &quot;distribution&quot;, etc. At this time, bar graphs are used; at the same time, it can also be displayed on a map based on geographic location data Different distribution characteristics;</p><p>The connection is mainly to check whether the two variables express the model relationship we expect to prove. For example, the expected sales may increase with the increase of the discount rate. At this time, it can be displayed with a bubble chart to express &quot;related to... &quot;, &quot;Growing with...&quot;, &quot;Differing with...&quot; the relationship between variables.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="data-reduction"></a>Data reduction<a aria-hidden="true" tabindex="-1" class="hash-link" href="#data-reduction" title="Direct link to heading">#</a></h2><p>The lecture provides a detailed introduction to data reduction related technologies, and also mentions some technologies of Data transform. Box-cox is also a common conversion technology.   </p><p>When considering reducing data, one has two primary options:<br>
<strong>1) Reducing the number of observations</strong><br>
<strong>2) Reducing the number of features</strong>   </p><p>Beyond doing this, one can change the &quot;resolution&quot; of the data, which means changing the extent to which each data point contains information. A good example of this would be decreasing the number of pixels in an image.   </p><p>When considering decreasing the number of observations, there are several things to consider. If one has a simple mode, they may only need a certain amount of observations/data to fit it, and there will be diminishing marginal returns to adding more data to train the model. If there is <em>no collinearity</em> between the features of the data set, a good rule of thumb is to require 10 observations per feature. If there is variable interaction, then you will require more data points.   </p><p>If you are considering dropping many data points, then you need to understand the fundamental principles behind <em>sampling theory</em>. For example - you do not need to sample all 350 million respondents of the United States in order to have a <em>representative sample</em> of the population. You could achieve this sample accuracy by ensuring certain qualities are met (which can be found <a href="https://www.sciencedirect.com/topics/social-sciences/sampling-theory" target="_blank" rel="noopener noreferrer">here</a>). </p><p>Compressed Sensing - a situation where it is possible to get a good model, despite having <em>less observations than the number of features</em>.   </p><p>In order to achieve a good model with this theory, you have to make two assumptions of the data:<br>
<strong>1) Sparsity of Signals</strong><br>
<strong>2) Incoherence</strong>   </p><p>While this theory predominatley relates to image processing, the information can best be summarized in these <a href="https://www.slideshare.net/ahmed_nasser_ahmed/introduction-to-compressive-sensing-43183358" target="_blank" rel="noopener noreferrer">slides</a>. </p><p>If you are considering removing features from the data in order to simplify it, an easy, and somewhat obtuse, assumption to have would be that you are removing information that is useful when removing features. There are many considerations here:<br>
1) Less variables can lead to more interpretable models<br>
2) Redundant Features can have collinearity - which lead to decreased return in the complexity VS information tradeoff.    </p><p>The &quot;curse of dimensionality&quot; also exists - where with increased levels of dimensions, as given by increased number of features, results in the lower ability for an indvidual to make inferences from the data, and or model results. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="addtional-resources"></a>Addtional Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#addtional-resources" title="Direct link to heading">#</a></h2><p><strong>Data Cleaning / Pre-Processing / Outliers</strong> </p><p>Box Plots and Outlier Detection using Python</p><p><a href="https://statinfer.com/104-3-5-box-plots-and-outlier-dectection-using-python/" target="_blank" rel="noopener noreferrer">https://statinfer.com/104-3-5-box-plots-and-outlier-dectection-using-python/</a></p><p>Boxplots in matplotlib: Markers and outliers</p><p><a href="https://stackoverflow.com/questions/17725927/boxplots-in-matplotlib-markers-and-outliers" target="_blank" rel="noopener noreferrer">https://stackoverflow.com/questions/17725927/boxplots-in-matplotlib-markers-and-outliers</a></p><p>Box Cox Transformation</p><p><a href="https://www.statisticshowto.com/box-cox-transformation/" target="_blank" rel="noopener noreferrer">https://www.statisticshowto.com/box-cox-transformation/</a></p><p>5 Regression Loss Functions All Machine Learners Should Know</p><p><a href="https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0" target="_blank" rel="noopener noreferrer">https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0</a></p><p>Huber Error| Loss Functions</p><p><a href="https://medium.com/@gobiviswaml/huber-error-loss-functions-3f2ac015cd45" target="_blank" rel="noopener noreferrer">https://medium.com/@gobiviswaml/huber-error-loss-functions-3f2ac015cd45</a></p><p>Focus on PYOD</p><p><a href="https://rsipvision.com/ComputerVisionNews-2019March/18/" target="_blank" rel="noopener noreferrer">https://rsipvision.com/ComputerVisionNews-2019March/18/</a></p><p>An Awesome Tutorial to Learn Outlier Detection in Python using PyOD Library</p><p><a href="https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/" target="_blank" rel="noopener noreferrer">https://www.analyticsvidhya.com/blog/2019/02/outlier-detection-python-pyod/</a></p><p><strong>Data Visualization</strong></p><p>The Extreme Presentation(tm) Method</p><p><a href="https://extremepresentation.typepad.com/blog/2006/09/choosing_a_good.html" target="_blank" rel="noopener noreferrer">https://extremepresentation.typepad.com/blog/2006/09/choosing_a_good.html</a></p><p><strong>Data Reduction</strong></p><p>Sampling Theory</p><p><a href="https://www.sciencedirect.com/topics/social-sciences/sampling-theory" target="_blank" rel="noopener noreferrer">https://www.sciencedirect.com/topics/social-sciences/sampling-theory</a></p><p>Introduction to compressive sensing</p><p><a href="https://www.slideshare.net/ahmed_nasser_ahmed/introduction-to-compressive-sensing-43183358" target="_blank" rel="noopener noreferrer">https://www.slideshare.net/ahmed_nasser_ahmed/introduction-to-compressive-sensing-43183358</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-22T00:55:23.000Z" class="docLastUpdatedAt_217_">10/21/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-10-dp"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Data Preprocessing</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-01-gaussian-dist"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">The Gaussian Distribution Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#overview-of-lecture" class="table-of-contents__link">Overview of Lecture</a></li><li><a href="#data-cleaning" class="table-of-contents__link">Data cleaning</a><ul><li><a href="#dealing-with-missing-values" class="table-of-contents__link">Dealing with Missing Values</a></li><li><a href="#handling-outliers" class="table-of-contents__link">Handling Outliers</a></li><li><a href="#huber-loss" class="table-of-contents__link">Huber Loss</a></li><li><a href="#python-outlier-detection-pyod" class="table-of-contents__link">Python Outlier Detection (PyOD)</a></li></ul></li><li><a href="#exploratory-data-analysisdata-visualization" class="table-of-contents__link">Exploratory Data Analysis/Data Visualization</a></li><li><a href="#data-reduction" class="table-of-contents__link">Data reduction</a></li><li><a href="#addtional-resources" class="table-of-contents__link">Addtional Resources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.4ee423d4.js"></script>
<script src="/APM-2020/runtime~main.9ef0f007.js"></script>
<script src="/APM-2020/main.a4e4d34c.js"></script>
<script src="/APM-2020/1.50c3da69.js"></script>
<script src="/APM-2020/2.84c4d38d.js"></script>
<script src="/APM-2020/30.e9a85367.js"></script>
<script src="/APM-2020/f976f453.0e2787a5.js"></script>
<script src="/APM-2020/17896441.8643754d.js"></script>
<script src="/APM-2020/eb5e4ebc.19f9dbb1.js"></script>
</body>
</html>