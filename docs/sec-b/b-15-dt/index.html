<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Classification - Decision Trees and Bayes Decision Theory | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Classification - Decision Trees and Bayes Decision Theory | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Justin Wagers, Jocelyne Walker, Rongzhi Xu, Yikang Wang. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Justin Wagers, Jocelyne Walker, Rongzhi Xu, Yikang Wang. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-15-dt"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-15-dt"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.32f049b1.js" as="script">
<link rel="preload" href="/APM-2020/main.8b7f41c7.js" as="script">
<link rel="preload" href="/APM-2020/1.e4c79b7e.js" as="script">
<link rel="preload" href="/APM-2020/2.9a4f08f3.js" as="script">
<link rel="preload" href="/APM-2020/37.aad1e583.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.d8642ff2.js" as="script">
<link rel="preload" href="/APM-2020/17896441.5ece2e3d.js" as="script">
<link rel="preload" href="/APM-2020/2b8d623c.ed2af4c7.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">🌜</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">🌞</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Classification - Decision Trees and Bayes Decision Theory</h1></header><div class="markdown"><p>Authors: <a href="https://www.linkedin.com/in/justin-wagers-47a5a5b3/" target="_blank" rel="noopener noreferrer">Justin Wagers</a>, <a href="https://www.linkedin.com/in/jocelynewalker/" target="_blank" rel="noopener noreferrer">Jocelyne Walker</a>, <a href="https://www.linkedin.com/in/rongzhi-xu-79b050117/" target="_blank" rel="noopener noreferrer">Rongzhi Xu</a>, <a href="https://www.linkedin.com/in/yikang-wang/" target="_blank" rel="noopener noreferrer">Yikang Wang</a>. (<a target="_blank" href="/APM-2020/assets/files/b-15-dt-6b613ba5f89aee15f686ce19de2048f3.pdf">PDF</a>)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="classification"></a>Classification<a aria-hidden="true" tabindex="-1" class="hash-link" href="#classification" title="Direct link to heading">#</a></h2><p>We learned about two main ways of classification: decision trees and statistical pattern recognition.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="decision-trees"></a>Decision Trees<a aria-hidden="true" tabindex="-1" class="hash-link" href="#decision-trees" title="Direct link to heading">#</a></h2><p>Decision trees are a simple and intuitive classification model that use explicit rules to partition the feature space. In feature space, we can visually represent these partitions with vertical and horizontal splits:</p><p><img src="https://imgur.com/Z1Eefo7.png"></p><p>For a simple decision tree example, consider a classification dataset in which we aim to classify individuals “yes” or “no” depending on whether they will purchase a computer. An example decision tree model to solve this problem is shown below:</p><p><img src="https://imgur.com/rF41j01.png"></p><p><strong>Decision boundaries:</strong></p><ul><li>Explicit: using the original features to set the boundaries</li><li>Implicit: Partitioning via discriminant functions (using transformed features)</li></ul><p><strong>Determining which splits to make first:</strong>
The solution above asserts the first split to be on whether the individual is a student. When determining which splits to make first, we would like to choose splits that are the most efficient, i.e. splits that reduce the disorder by the highest amount possible. There are two particularly popular methods to measure this disorder:</p><p><img src="https://imgur.com/F1OAOWP.png"></p><p>Both are metrics of heterogeneity and aim to split the data into more homogeneous subsets. In practice, these two metrics often result in similar models and are often used interchangeably. </p><p><strong>Determining how many splits to make:</strong>
Our main goal with decision trees is to obtain a small, shallow tree with low uncertainty at the splits. While you could obtain a 100% classification rate on training data with enough splits, this will cause overfitting and lead to higher test error. Thus, it is important to validate the model to determine the number of splits that will give the highest test accuracy. This process is often called “growing and pruning.”</p><p><img src="https://imgur.com/LtRQ3q8.png"></p><p><strong>Tree Size:</strong>
Overfitting is a significant issue for the decision tree model and it is essential to find the right size of the tree.</p><ul><li>Grow big: use a greedy, recursive forward search to build a big tree. <ul><li>Start the tree that is a single node, then search over all possible decision rule to find the one that gives the biggest decrease in loss (increase in fit).</li></ul></li><li>Prune Back: decrease the size of decision tree.<ul><li>Pre-Pruning: stop growing the tree earlier, before the biggest decrease in loss.</li><li>Post-Pruning: allows the tree to perfectly grow, and then prune back the nodes of the big tree.</li></ul></li></ul><p><strong>Expanding Upon the Decision Tree</strong></p><p>The decision tree model on its own is easily understood, but often provides lower classification accuracy than desired. However, there some popular expansions on the decision tree model that can lead to more robust and accurate models: </p><ul><li><strong>Random Forest:</strong> This method essentially compounds many different decision trees in the theory that in unison, they will perform better than any single model on its own. </li><li><strong>Gradient Boosting:</strong> Another method of improving the performance of a single decision tree, gradient boosting builds on smaller trees as opposed to the fully fleshed out trees in random forests, adding one classifier at a time to improve the model in each step forward. Essentially, gradient boosting continually adds more trees while random forest synthesizes many trees at once. </li></ul><p><img src="https://imgur.com/D2b3fEK.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bayes-decision-theory"></a>Bayes Decision Theory<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bayes-decision-theory" title="Direct link to heading">#</a></h2><p>Bayes&#x27; Theorem is fundamental to statistical analysis, and it&#x27;s considered the ideal pattern classifier because its decision rule <strong>automatically minimizes its loss function.</strong> </p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="a-review-of-bayes-theorem"></a>A Review of Bayes&#x27; Theorem<a aria-hidden="true" tabindex="-1" class="hash-link" href="#a-review-of-bayes-theorem" title="Direct link to heading">#</a></h4><p>Here&#x27;s Bayes&#x27; Theorem:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant="normal">∣</mi><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mi mathvariant="normal">∣</mi><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi><mo>=</mo><mfrac><mrow><mi>l</mi><mi>i</mi><mi>k</mi><mi>e</mi><mi>l</mi><mi>i</mi><mi>h</mi><mi>o</mi><mi>o</mi><mi>d</mi><mo>∗</mo><mi>p</mi><mi>r</mi><mi>i</mi><mi>o</mi><mi>r</mi></mrow><mrow><mi>e</mi><mi>v</mi><mi>i</mi><mi>d</mi><mi>e</mi><mi>n</mi><mi>c</mi><mi>e</mi></mrow></mfrac></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} p(C_k |x) = \frac{p(x|C_k)p(C_k)}{p(x)} = posterior = \frac{likelihood * prior}{evidence} \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.6630000000000003em;vertical-align:-1.0815000000000001em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5815000000000001em"><span style="top:-3.5815em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">∣</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal">o</span><span class="mord mathnormal">s</span><span class="mord mathnormal">t</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.3714399999999998em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord mathnormal">i</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal">n</span><span class="mord mathnormal">c</span><span class="mord mathnormal">e</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal" style="margin-right:0.03148em">k</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">i</span><span class="mord mathnormal">h</span><span class="mord mathnormal">o</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">p</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.686em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0815000000000001em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>You&#x27;ve probably seen it many times before. But what does it really mean to us for classification problems?</p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="example"></a>Example<a aria-hidden="true" tabindex="-1" class="hash-link" href="#example" title="Direct link to heading">#</a></h4><p>Let&#x27;s put this in terms of a real-world example. Let&#x27;s assume we have some data about GMAT scores and are attempting to classify students as either UT MBA students or UT MSBA students. </p><p>Here&#x27;s our probability distribution for each class.
<img src="https://imgur.com/3hDOqoZ.png"></p><p>Let&#x27;s assume we&#x27;re given a student with a GMAT of 710, and we want to predict whether they&#x27;re an MSBA or an MBA student. To do this, we need to calculate two conditional probabilities: </p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>M</mi><mi>S</mi><mi>B</mi><mi>A</mi><mi mathvariant="normal">∣</mi><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mi mathvariant="normal">∣</mi><mi>M</mi><mi>S</mi><mi>B</mi><mi>A</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>p</mi><mo stretchy="false">(</mo><mi>M</mi><mi>S</mi><mi>B</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} p(MSBA | GMAT = 710) = \frac{p(GMAT = 710 | MSBA)*p(MSBA)}{p(GMAT = 710)} \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.6630000000000003em;vertical-align:-1.0815000000000001em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5815000000000001em"><span style="top:-3.5815em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0815000000000001em"><span></span></span></span></span></span></span></span></span></span></span></span></div><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>M</mi><mi>B</mi><mi>A</mi><mi mathvariant="normal">∣</mi><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mi mathvariant="normal">∣</mi><mi>M</mi><mi>B</mi><mi>A</mi><mo stretchy="false">)</mo><mo>∗</mo><mi>p</mi><mo stretchy="false">(</mo><mi>M</mi><mi>S</mi><mi>B</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mo stretchy="false">)</mo></mrow></mfrac></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned} p(MBA | GMAT = 710) = \frac{p(GMAT = 710 | MBA)*p(MSBA)}{p(GMAT = 710)} \end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.6630000000000003em;vertical-align:-1.0815000000000001em"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5815000000000001em"><span style="top:-3.5815em"><span class="pstrut" style="height:3.427em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span><span class="mord">∣</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em"><span style="top:-2.314em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.677em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mord">∣</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">∗</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal" style="margin-right:0.05764em">S</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord mathnormal">A</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0815000000000001em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p><img src="https://imgur.com/WVDOrG5.png"></p><p>So the question is, given a GMAT of 710, which class do we predict?</p><p>The theoretical answer to this is in Bayes Decision Rule, which states that: $ i = \underset{j = 1 ... K}{argmax}{P(C_j |x)}$ where <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.07153em">K</span></span></span></span></span> is the number of classes.</p><p>Thus, in our example, we assign class K to GMAT score of 710 to whichever class K has the highest value of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>K</mi><mi mathvariant="normal">∣</mi><mi>G</mi><mi>M</mi><mi>A</mi><mi>T</mi><mo>=</mo><mn>710</mn><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(K | GMAT = 710)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07153em">K</span><span class="mord">∣</span><span class="mord mathnormal">G</span><span class="mord mathnormal" style="margin-right:0.10903em">M</span><span class="mord mathnormal">A</span><span class="mord mathnormal" style="margin-right:0.13889em">T</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">7</span><span class="mord">1</span><span class="mord">0</span><span class="mclose">)</span></span></span></span></span> From the image below, you see that at GMAT = 710, the MSBA class has the higher probability. </p><p><img src="https://imgur.com/3Ju11Gm.png"></p><p>We see the irreducible error from this theory in the image below.</p><p><img src="https://imgur.com/JAspXYZ.png"></p><p>So <strong>why is this optimal?</strong> If each class is chosen to that condition risk for <em>each input</em> is minimized, then <em>overall risk</em> is minimized too. </p><p>However, it&#x27;s important to remember the <strong>assumptions</strong> of this Bayesian model, which might not always hold:</p><ul><li><p>Decision problem must be posed in probabilitistic terms</p></li><li><p>All relevant probability values are known</p></li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="additional-resources"></a>Additional Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#additional-resources" title="Direct link to heading">#</a></h2><ul><li><p><a href="https://www.projectrhea.org/rhea/index.php/Bayesian_Decision_Theory" target="_blank" rel="noopener noreferrer">Bayesian Decision Theory - Mathematical Explanation</a></p></li><li><p><a href="https://towardsdatascience.com/bayesian-decision-theory-81103a68978e" target="_blank" rel="noopener noreferrer">Bayesian Decision Theory - Graphical Explanation</a></p></li><li><p><a href="https://www.youtube.com/watch?v=HZGCoVF3YvM" target="_blank" rel="noopener noreferrer">Bayes Rule Video</a></p></li></ul></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-11-02T23:59:54.000Z" class="docLastUpdatedAt_217_">11/2/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-14-pca"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« Principal Component Analysis (PCA)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#classification" class="table-of-contents__link">Classification</a></li><li><a href="#decision-trees" class="table-of-contents__link">Decision Trees</a></li><li><a href="#bayes-decision-theory" class="table-of-contents__link">Bayes Decision Theory</a></li><li><a href="#additional-resources" class="table-of-contents__link">Additional Resources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright © 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.32f049b1.js"></script>
<script src="/APM-2020/main.8b7f41c7.js"></script>
<script src="/APM-2020/1.e4c79b7e.js"></script>
<script src="/APM-2020/2.9a4f08f3.js"></script>
<script src="/APM-2020/37.aad1e583.js"></script>
<script src="/APM-2020/f976f453.d8642ff2.js"></script>
<script src="/APM-2020/17896441.5ece2e3d.js"></script>
<script src="/APM-2020/2b8d623c.ed2af4c7.js"></script>
</body>
</html>