<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Bias-Variance Dilemma | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Bias-Variance Dilemma | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Zachary Hall, Mac Husted, Nicole Jiang"><meta data-react-helmet="true" property="og:description" content="Authors: Zachary Hall, Mac Husted, Nicole Jiang"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-5-bias-variance"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-5-bias-variance"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.38b57550.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.c423b8d2.js" as="script">
<link rel="preload" href="/APM-2020/main.b2d97c18.js" as="script">
<link rel="preload" href="/APM-2020/1.d0ed0340.js" as="script">
<link rel="preload" href="/APM-2020/2.57c1464c.js" as="script">
<link rel="preload" href="/APM-2020/28.5af0e23c.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.a868d95a.js" as="script">
<link rel="preload" href="/APM-2020/17896441.2a24b6b8.js" as="script">
<link rel="preload" href="/APM-2020/2849db98.6f5009ff.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-3-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-5-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-6-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-7-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-8-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-9-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-3-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-4-regularization">Regularization</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-5-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-6-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-7-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-8-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-9-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Bias-Variance Dilemma</h1></header><div class="markdown"><p>Authors: Zachary Hall, Mac Husted, Nicole Jiang</p><p>Expected square loss (E[L]) formula:</p><p><img src="/APM-2020/assets/images/APM1-3339ad9c723452d08ffc02536d1903d0.png"></p><ul><li>t: target value (random variable) â†’ Assume: t = h (x) + zero - mean noise</li><li>x: input value (random variable)</li><li>h(x): target line</li><li>y(x): predicted line</li></ul><p>The first term of the expected square loss is bias and variance. It is the difference between the predicted line and the target line. This first term is composed of bias^2 + variance.</p><p>The second term of the expected square loss represents the variance of the error term. It shows the difference between target line and the target points.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="math-details-the-bias-variance-decomposition-optional"></a>Math Details: The Bias-Variance Decomposition (optional)<a aria-hidden="true" tabindex="-1" class="hash-link" href="#math-details-the-bias-variance-decomposition-optional" title="Direct link to heading">#</a></h2><p><img src="/APM-2020/assets/images/APM2-bde3cd1548b6c24109b80f1aa0301618.png"></p><p>In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves <em>low variance</em> and <em>low bias</em>.</p><p>Expected loss = (bias)^2 + variance + noise</p><p><img src="/APM-2020/assets/images/APM3-088b39227d10d56edf41a1835f79ce4d.png"></p><p><strong>Variance</strong> : how sensitive the model is to variations in data.</p><p>Variance refers to the amount by which Ë†<em>f</em> would change if we estimated it using a different data set (training dataset &amp; testing dataset).</p><p>In general, more flexible statistical methods have higher variance.</p><p><img src="/APM-2020/assets/images/APM4-0f08343574ec60d18508ea68f9d3f740.png"></p><p><strong>Bias</strong> : how good the average model is;</p><p>Bias refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model.</p><p>Generally, more flexible methods result in less bias.</p><p><img src="/APM-2020/assets/images/APM5-ab48a01b5e38a298a1245e3e0155a6df.png"></p><p>** the bias and variance concepts here apply to a predictive model, rather than to an estimator of a specific value.</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bias-variance-tradeoff"></a>Bias-Variance Tradeoff<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bias-variance-tradeoff" title="Direct link to heading">#</a></h1><p>When a model is too simple and has very few parameters then it may have high bias and low variance. While, if a model is too complex and has too many parameters then it&#x27;s going to have too much variance and low bias. The optimal model is one where the level of complexity leads equivalent reduction in variance and increase in bias.</p><p>If our model complexity exceeds this optimal spot, the model will be in effect over-fit. On the other hand, if the model is too simple, the model will be under-fit . Thus, one has to ensure that they balance the complexity of the model that will provide the reduction in both bias and variance.</p><p><img src="/APM-2020/assets/images/APM6-6d7794eb3b9dd81155ffad2ef330f293.png"></p><p><img src="/APM-2020/assets/images/APM7-73c30bc6d83e7edf5491719bba92cec7.png"></p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="how-does-one-improve-their-model"></a>How does one improve their model?<a aria-hidden="true" tabindex="-1" class="hash-link" href="#how-does-one-improve-their-model" title="Direct link to heading">#</a></h1><p>The first method to improve one&#x27;s model is to get more training data. This will reduce the variance of one&#x27;s estimates while keeping the bias overall the same. It also has an additional benefit of allowing one to increase their model complexity. The more training data one has the more likely that training error and the test error will converge. Since the test error is an unbiased estimate of the true mean, the model will be closer to the &quot;true&quot; error.</p><p>The second method is to change the complexity of the model via regularization. Through regularization, one can find the optimal complexity that will reduce the bias and variance allowing one to improve their model&#x27;s overall fit.</p><p>The third method is to change the optimization method. Through cross validation and regularization one can also optimize the parameters of the model which will also improve the overall fit of the model.</p><p>The last method is to change the model type. There are times when a linear model may not provide the most optimal solution. Sometimes one might change the model type to a nonlinear model which may improve the overall fit of the model.</p><p>If these methods don&#x27;t improve the overall solution, one might need to do feature engineering. This involved careful selection and possible manipulation of the data features. The reason to do this is to feed the model only the most optimal inputs. If one can consistently give the model only part of the data it needs to make more accurate predictions then it doesn&#x27;t have to deal with any extra noise that comes from the rest of data. It also allows one to reduce the possibility that the model is suffering from the collinearity problem. An alternative way of selecting features may be through principal component analysis for example.</p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="the-effect-of-regularization-on-bias-and-variance"></a>The Effect of Regularization on Bias and Variance<a aria-hidden="true" tabindex="-1" class="hash-link" href="#the-effect-of-regularization-on-bias-and-variance" title="Direct link to heading">#</a></h1><p>When one regularizes a model, they are penalizing model complexity with the goal of reducing the variance and bias to an optimal level. When there is more regularization (i.e. higher lambda or alpha), there is more bias and less variance. When one regularizes a model, they must keep this in mind to ensure that they haven&#x27;t regularized the model too much where more bias is being introduced. This goes back to the idea of the bias-variance trade-off. When we keep this mind when regularizing, the model should make more reliable predictions on future data sets other than just the training dataset.</p><p><img src="/APM-2020/assets/images/APM9-d5818475290eef08c2967ad447d43929.png"></p><h1><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="additional-information--resources"></a>Additional Information &amp; Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#additional-information--resources" title="Direct link to heading">#</a></h1><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="irreducible-error"></a>Irreducible error<a aria-hidden="true" tabindex="-1" class="hash-link" href="#irreducible-error" title="Direct link to heading">#</a></h2><p>Variability associated with error terms also affects the accuracy of our predictions. This is known as the <em> <strong>irreducible</strong> </em> error, because no matter how well we estimate <em>f</em>, we cannot reduce the error introduced by the error terms.</p><p><strong>Why is the irreducible error larger than zero?</strong></p><p>The quantity may contain <strong>unmeasured variables</strong> that are useful in predicting <em>Y</em> : since we don&#x27;t measure them, <em>f</em> cannot use them for its prediction.</p><p>The quantity may also contain <strong>unmeasurable variation</strong>. For example, the risk of an adverse reaction for a given patient on a given day.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="resources"></a>Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#resources" title="Direct link to heading">#</a></h2><p><a href="https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229" target="_blank" rel="noopener noreferrer">An article on bias-variance trade-off</a></p><p><a href="https://towardsdatascience.com/3-ways-to-improve-your-machine-learning-results-without-more-data-f2f0fe78976e" target="_blank" rel="noopener noreferrer">An article on improving one&#x27;s model</a></p><p><a href="http://www.ds100.org/sp18/assets/lectures/lec17/17-bias-variance-regularization.pdf" target="_blank" rel="noopener noreferrer">An article on regularization and bias and variance</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-09-28T21:28:29.000Z" class="docLastUpdatedAt_217_">9/28/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-4-regularization"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Regularization</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-6-gradient-descent"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Beyond Linear Regression and Gradient Descent Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#math-details-the-bias-variance-decomposition-optional" class="table-of-contents__link">Math Details: The Bias-Variance Decomposition (optional)</a></li><li><a href="#irreducible-error" class="table-of-contents__link">Irreducible error</a></li><li><a href="#resources" class="table-of-contents__link">Resources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.38b57550.js"></script>
<script src="/APM-2020/runtime~main.c423b8d2.js"></script>
<script src="/APM-2020/main.b2d97c18.js"></script>
<script src="/APM-2020/1.d0ed0340.js"></script>
<script src="/APM-2020/2.57c1464c.js"></script>
<script src="/APM-2020/28.5af0e23c.js"></script>
<script src="/APM-2020/f976f453.a868d95a.js"></script>
<script src="/APM-2020/17896441.2a24b6b8.js"></script>
<script src="/APM-2020/2849db98.6f5009ff.js"></script>
</body>
</html>