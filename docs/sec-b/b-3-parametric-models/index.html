<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Function approximation (Regression) | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Function approximation (Regression) | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Rehan Daya, Manan Desai, Samir Epili"><meta data-react-helmet="true" property="og:description" content="Authors: Rehan Daya, Manan Desai, Samir Epili"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-3-parametric-models"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-3-parametric-models"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.38b57550.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.1cc83bc5.js" as="script">
<link rel="preload" href="/APM-2020/main.2f8acb99.js" as="script">
<link rel="preload" href="/APM-2020/1.50c3da69.js" as="script">
<link rel="preload" href="/APM-2020/2.84c4d38d.js" as="script">
<link rel="preload" href="/APM-2020/30.dbf5b97c.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.396d1220.js" as="script">
<link rel="preload" href="/APM-2020/17896441.bf71f1c2.js" as="script">
<link rel="preload" href="/APM-2020/c29aeb71.775d6028.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-3-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-5-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-6-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-7-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-8-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-9-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-3-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-5-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-6-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-7-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-8-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-9-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Function approximation (Regression)</h1></header><div class="markdown"><p>Authors: Rehan Daya, Manan Desai, Samir Epili</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="parametric-models"></a>Parametric Models<a aria-hidden="true" tabindex="-1" class="hash-link" href="#parametric-models" title="Direct link to heading">#</a></h3><p>A parametric model is a particular class of statistical models such as linear regression. A parametric model is considered linear if it can be written like this: </p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>0</mn></mrow><mi>M</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub><msub><mi>Ï•</mi><mi>i</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>Ï•</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">y(x,w)= \sum_{i=0}^{M} w_i\phi_i(x) = w^T\phi(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.2809409999999999em;vertical-align:-0.29971000000000003em"></span><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em">âˆ‘</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.981231em"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">0</span></span></span></span><span style="top:-3.2029em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.29971000000000003em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal">Ï•</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1.0913309999999998em;vertical-align:-0.25em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">Ï•</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span></p><p>where <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>T</mi></mrow><annotation encoding="application/x-tex">y=T</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.13889em">T</span></span></span></span></span>=Target, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi><mo>=</mo><mi>Ï•</mi></mrow><annotation encoding="application/x-tex">x=\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em"></span><span class="mord mathnormal">Ï•</span></span></span></span></span>=Transformed input,  <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>y</mi><mo>^</mo></mover><mo>=</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">\hat{y}=y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.19444em"><span class="mord">^</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.03588em">y</span></span></span></span></span>=Answer, <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î²</mi><mo>=</mo><mi>w</mi></mrow><annotation encoding="application/x-tex">\beta=w</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.05278em">Î²</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.02691em">w</span></span></span></span></span>=Weight.Specifically, a parametric model is a family of probability distributions that has a finite number of parameters. </p><p>Non-parametric Models are statistical models that do not often conform to a normal distribution, as they rely upon continuous data, rather than discrete values. A good example of this is K-Nearest Neighbors.</p><p>When making the parametric model we have certain assumptions:</p><ol><li>Expected value of T given the basis function vector phi is linear in phi.</li><li>All distributions around the expected values are assumed to be i.i.d. zero mean Gaussian with constant variance.</li><li>Minimizing Mean Squared Error (MSE) on the training data yields the Maximum Likelihood Estimate (MLE) solution of the assumed generative model.
</li></ol><p>To ensure the parametric model is accurate we try to minimize the SSE (Sum-of-Squares Error) by utilizing the RMSE (Root Mean Squared Error):</p><p><img src="https://images.squarespace-cdn.com/content/v1/58c95854c534a56689231265/1531152039771-PI1ZAS4DSZCCUBST3KEK/ke17ZwdGBToddI8pDm48kC1MXajuusIgG2-0QRxSgZpZw-zPPgdn4jUwVcJE1ZvWhcwhEtWJXoshNdA9f1qD7dso8WS9HrXe-DDzLfr_qHmdYaYQTvCmLudhQgTG6nPRZVy99kA3NNj9L0tcpT6_qQ/Formula.PNG?format=300w"> </p><p>This is essentially the difference between the target variable and your prediction. As this value shrinks your R-Squared will increase. </p><p>The takeaway here is:</p><p><em>There is an exact closed form solution for the optimal weights for a linear model. Assuming the inverse of a matrix exists, so the determinant cant be 0.</em></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="collinearity"></a>Collinearity<a aria-hidden="true" tabindex="-1" class="hash-link" href="#collinearity" title="Direct link to heading">#</a></h3><p>In statistics, multicollinearity is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy. We tend to drop a variable that is collinear with another variable and test the RMSE change. Then repeat the step with all other collinear variables to see which combination of variables provides the least amount of collinearity and complexity.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="fitting"></a>Fitting<a aria-hidden="true" tabindex="-1" class="hash-link" href="#fitting" title="Direct link to heading">#</a></h3><p>Least Squares which uses euclidean distance is utilized to fit the model to the data. A good example is seen below:</p><p><img src="https://i0.wp.com/statisticsbyjim.com/wp-content/uploads/2017/04/flp_linear.gif?resize=576%2C384"> </p><p>You can try higher polynomial functions which will cause odd looking models to form versus this straight line. But the only way to know if it fits well is to look at the out-of-sample RMSE. Many times we look at in-sample RMSE which leads to high bias since the model is trained on the training dataset and tested on it too. Be wary though, that the higher order polynomial your function is, the more complex your model becomes.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="a-deeper-dive-into-parametric-models"></a>A Deeper Dive into Parametric Models<a aria-hidden="true" tabindex="-1" class="hash-link" href="#a-deeper-dive-into-parametric-models" title="Direct link to heading">#</a></h3><p>There are several different ways to map input variables to output variables. However, there is a lot standing in the way of making an accurate prediction. In order to simplify the learning process, algorithms can make certain assumptions. These assumptions come in the form of simplifying the function to a known form. This known form has a set of parameters, or weights, of <em>fixed</em> size. The number of weights does not change with respect to the size of the data. </p><p>Models that make these assumptions are called parametric models. Parametric algorithms take two big steps:</p><ol><li>The algorithm will pick a form for the function</li><li>The algorithm will attempt to learn the coefficients for the weights from the training set of data by finding the Maximum Likelihood Estimate solution.
</li></ol><p>We have seen Maximum Likelihood Estimation already so I won&#x27;t rehash it.</p><p>Parametric models come with benefits and drawbacks. The main benefit of parametric models is how simple they are. They are easy to understand, and the existence of weights allows for easier interpretability. Consider, for example, a multiple linear regression model. Once the model is fit, the weights for each input variable clearly show the effect of that variable on the target. Another benefit that stems from simplicity is speed. These models can be fit very quickly. Additionally, they do not require a large amount of data. These models can work well in simple situations even with limited training data.</p><p>However, there are many downsides to parametric models. Parametric models are, by definition, limited to its parameters and its form. Simply by choosing a form and a number of parameters, the model is already handicapped if the true underlying form of the data is different. This can lead to poor fit in many scenarios. Additionally, parametric models are inherently limited in complexity. There are only so many model forms from which to choose, and once they are exhausted, parametric algorithms have nothing else to try.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="a-deeper-dive-into-model-complexity-and-overfitting"></a>A Deeper Dive into Model Complexity and Overfitting<a aria-hidden="true" tabindex="-1" class="hash-link" href="#a-deeper-dive-into-model-complexity-and-overfitting" title="Direct link to heading">#</a></h3><p>At this point in the course, we have observed that there exists a tradeoff between a more complex model with several features and parameters, and the overfitting of test data. Simply finding the optimal parameters for a model such that every point in a training set is fit perfectly can provide diminishing returns for multiple reasons.
The issue for simply fitting the most complex model is the reduction in signal-to-noise ratio. Noise is the randomness present in all raw collected data, while the signal consists of the points whose pattern the model is being used to discern. Because noise is random in nature it is important to select a model that is flexible enough to adapt to different test data, but not to the point that it follows noise and reduces interpretability.  </p><p><img src="https://tutorialspoint.dev/image/t0zit.png"></p><p>For instance, the image above shows parametric models of increase complexity from left to right. By simply increasing the number of variables, or features, the models can fit data points more accurately. However, the model on the right becomes too sensitive and suffers from high variance due to weighting of the additional variables. Conversely, the model on the left is too simple and suffers from high bias, such as assuming non-linear data is linear. This introduces the bias-variance tradeoff argument. Because we will be discussing this tradeoff more in detail in later lectures, only a brief overview will be provided here. </p><p><img src="https://elitedatascience.com/wp-content/uploads/2017/06/Bias-vs.-Variance-v4-chart.png"></p><p>Since squared bias and variance are inversely related, simpler and more biased models will likely underperform the same as very complex and highly flexible models on test data. Additionally, the total error is the sum of squared bias, variance, and irreducible error. Thus, an optimal model will be at the point where the total error is minimized. Methods for obtaining this optimal regression model include using more data to train more complex models, regularization, and cross validation.</p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-09-23T22:02:36.000Z" class="docLastUpdatedAt_217_">9/23/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-2-mlr"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Multiple Linear Regression</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-4-regularization"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Regularization Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#parametric-models" class="table-of-contents__link">Parametric Models</a></li><li><a href="#collinearity" class="table-of-contents__link">Collinearity</a></li><li><a href="#fitting" class="table-of-contents__link">Fitting</a></li><li><a href="#a-deeper-dive-into-parametric-models" class="table-of-contents__link">A Deeper Dive into Parametric Models</a></li><li><a href="#a-deeper-dive-into-model-complexity-and-overfitting" class="table-of-contents__link">A Deeper Dive into Model Complexity and Overfitting</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.38b57550.js"></script>
<script src="/APM-2020/runtime~main.1cc83bc5.js"></script>
<script src="/APM-2020/main.2f8acb99.js"></script>
<script src="/APM-2020/1.50c3da69.js"></script>
<script src="/APM-2020/2.84c4d38d.js"></script>
<script src="/APM-2020/30.dbf5b97c.js"></script>
<script src="/APM-2020/f976f453.396d1220.js"></script>
<script src="/APM-2020/17896441.bf71f1c2.js"></script>
<script src="/APM-2020/c29aeb71.775d6028.js"></script>
</body>
</html>