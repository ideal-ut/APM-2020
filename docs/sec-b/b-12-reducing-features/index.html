<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">All About Features | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="All About Features | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Matthew Ruffner, Ali Daanesh Sayyed, Paridhi Sheth. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Matthew Ruffner, Ali Daanesh Sayyed, Paridhi Sheth. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-12-reducing-features"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-12-reducing-features"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.ee71a697.js" as="script">
<link rel="preload" href="/APM-2020/main.34e32ce5.js" as="script">
<link rel="preload" href="/APM-2020/1.433d51c7.js" as="script">
<link rel="preload" href="/APM-2020/2.8e74170b.js" as="script">
<link rel="preload" href="/APM-2020/38.a3af94e3.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.4a8df433.js" as="script">
<link rel="preload" href="/APM-2020/17896441.b66b5b0d.js" as="script">
<link rel="preload" href="/APM-2020/a26b5826.e593d911.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-16-loss">Misclassification Rate and Minimizing Expected Loss</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">All About Features</h1></header><div class="markdown"><p>Authors: Matthew Ruffner, Ali Daanesh Sayyed, Paridhi Sheth. (<a target="_blank" href="/APM-2020/assets/files/b-12-reducing-features-6ac4160e1af06ff3b747fe5be1919c1a.pdf">PDF</a>)</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="summary"></a>Summary<a aria-hidden="true" tabindex="-1" class="hash-link" href="#summary" title="Direct link to heading">#</a></h3><p>The lecture covered the concepts of curse of  dimensionality, feature extraction, selection and selecting a subset of features. Curse of dimensionality is the phenomena that arises when analyzing and organizing data in high dimensional spaces. Feature extraction is the process of dimensionality reduction by which an initial set of raw data is reduced to manageable groups for processing, leading to an increase in model accuracy. Moreover feature selection is the process of selecting features that contribute to the variable one is predicting. Lastly selecting a subset of features is the process of selecting the best combination of features for a model.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="curse-of-dimensionality"></a>Curse of Dimensionality<a aria-hidden="true" tabindex="-1" class="hash-link" href="#curse-of-dimensionality" title="Direct link to heading">#</a></h3><p>As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially - the more the features, the more the data. The dimensions can be increased 10^d which increases the data that many folds and decreases the accuracy. For example: figure a) shows 10 data points in 1-D meaning there is only one feature in the dataset. It can easily be illustrated on a line with only 10 values (x=1,2,3,4...10). But, if we add one more feature, the same data will be illustrated in 2-D represented in figure b) leading to an increase in dimension space 10^2=100. Lastly, for figure c) dimension space increases 10^3=1000. As dimension grows, dimension space increases exponentially. </p><p>This exponential growth in data causes high sparsity in the data set and unnecessarily increases storage space and processing time for the particular modelling algorithm. For example, in image recognition there is a problem of high resolution images 1280 Ã— 720 = 921,600 pixels i.e. 921600 dimensions. This is called the curse of dimensionality. </p><p><strong>Problems with high dimensional data:</strong></p><ol><li>Increases the processing time</li><li>Over fitting</li><li>Required data size increases exponentially</li><li>Principal Component Analysis is one of the common methods used to reduce the dimensionality. The idea behind PCA is to find out dimensions which account for most of the variance within data.</li></ol><p><strong>Advantages of Dimensionality Reduction:</strong></p><ol><li>Computational Efficiency</li><li>Cost associated with collection and storage of huge data</li><li>Classification problem</li><li>Ease of interpretation</li></ol><p><img alt="Curse of Dimensionality" src="/APM-2020/assets/images/curse-dimensionality-2b9e7c2ade78f14749a6b14353838f96.png"></p><p><strong>References:</strong></p><ol><li><a href="https://www.youtube.com/watch?v=OyPcbeiwps8" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=OyPcbeiwps8</a> </li><li><a href="https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html" target="_blank" rel="noopener noreferrer">https://www.kdnuggets.com/2017/04/must-know-curse-dimensionality.html</a></li><li><a href="https://www.fromthegenesis.com/curse-of-dimensionality/" target="_blank" rel="noopener noreferrer">https://www.fromthegenesis.com/curse-of-dimensionality/</a></li></ol><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="feature-extraction"></a>Feature extraction<a aria-hidden="true" tabindex="-1" class="hash-link" href="#feature-extraction" title="Direct link to heading">#</a></h3><p>An analogy for feature extraction I found online is that models are like the motor of a car, and features are like the fuel. One needs to find features that represent the information in the data that will best fit the needs of the algorithm that is to be used in a model. Feature extraction reformats, combines, and transforms primary features into new ones in order to create a model that will achieve a desired goal.</p><p>Nowadays it is common to work with datasets that contain hundreds or even thousands of features, which is something to keep in mind when creating models, especially for cases where the amount of features are greater than the number of observations. Having such datasets can lead to models that suffer from overfitting. As data scientists we want to avoid our models from being affected from such errors, hence the concept of feature extraction. Feature extraction allows data scientists to avoid the problem mentioned above by reducing the number of dimensions used within the model creation process. </p><p>Feature extraction techniques are advantageous, and can help improve the accuracy of a model, reduce the risk of overfitting, speed up the training, improve the data visualization, as well as increase the explainability of a model. </p><p>The goal of feature extraction is to reduce the number of features in a dataset by creating new features from existing ones (while removing original features). These new features should summarize the information from the original features while discarding any information that appears less important. </p><p><strong>References:</strong></p><ol><li><a href="https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/feature-extraction-techniques-d619b56e31be</a></li><li><a href="https://quantdare.com/what-is-the-difference-between-feature-extraction-and-feature-selection/" target="_blank" rel="noopener noreferrer">https://quantdare.com/what-is-the-difference-between-feature-extraction-and-feature-selection/</a></li></ol><p><img alt="Feature Extraction vs Selection" src="/APM-2020/assets/images/features_extraction_vs_selection-e0c14063e2b0815418968d841eb56fcf.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="feature-selection"></a>Feature selection<a aria-hidden="true" tabindex="-1" class="hash-link" href="#feature-selection" title="Direct link to heading">#</a></h3><p>When one considers training a model there are various preprocessing steps that must be taken in order to arrive at an accurate, interpretable model. One such step is reducing unnecessary data from our original dataset which does not only simplify model creation but can actually even improve the said modelâ€™s accuracy as well as boost speed of computation.  Feature selection is a data reduction technique that pertains to choosing a subset of the original features thereby reducing the number of predictors. A subset of features can be derived from the original dataset by introducing a â€˜selector matrixâ€™ (also called a filter) consisting of ones and zeros and multiplying by the original features in vector form. </p><p>Selecting a subset of features
Selecting the best combination of features is classified as an np.hard problem (essentially just an insanely hard nondeterministic polynomial time problem that requires vast amounts of computing power to solve). So, when deciding which subset of features to use, we do so using heuristics which allow for an easier/ simpler solution but not necessarily the optimal one. There are a few ways to go about this, one possibility would be to use a filter method wherein intrinsic quality measures are compared such as the correlation between predictors (in this situation, if the predictors are strongly positively correlated, then one or many of the predictors may not be contributing as much to the seen variance in the model and can be eliminated). Another possibility are wrappers, an extrinsic evaluation technique such as greedy (only making use of some features)  evaluation of a predictive model e.g. a decision tree with a restricted depth. Other wrappers include stepwise and backward elimination (note: backward is generally less applicable than forward for large datasets because computing limitations) which either incrementally add or remove  variables from a model to optimize accuracy. Finally, there are embedded methods, wherein feature selection is naturally part of a model such as a LASSO model which adds a penalty to forcefully make coefficients equal to zero for large enough lambda.</p><p><strong>References:</strong></p><ol><li><a href="https://towardsdatascience.com/feature-selection-techniques-1bfab5fe0784" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/feature-selection-techniques-1bfab5fe0784</a></li></ol></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-24T00:57:10.000Z" class="docLastUpdatedAt_217_">10/23/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-11-outliers"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Handling Outliers</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-13-dd-docs"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Interactive Online Visualization Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary" class="table-of-contents__link">Summary</a></li><li><a href="#curse-of-dimensionality" class="table-of-contents__link">Curse of Dimensionality</a></li><li><a href="#feature-extraction" class="table-of-contents__link">Feature extraction</a></li><li><a href="#feature-selection" class="table-of-contents__link">Feature selection</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.ee71a697.js"></script>
<script src="/APM-2020/main.34e32ce5.js"></script>
<script src="/APM-2020/1.433d51c7.js"></script>
<script src="/APM-2020/2.8e74170b.js"></script>
<script src="/APM-2020/38.a3af94e3.js"></script>
<script src="/APM-2020/f976f453.4a8df433.js"></script>
<script src="/APM-2020/17896441.b66b5b0d.js"></script>
<script src="/APM-2020/a26b5826.e593d911.js"></script>
</body>
</html>