<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">The Gaussian Distribution | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="The Gaussian Distribution | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Luke Bravo, Sidd Chauhan and Shuming Chen. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Luke Bravo, Sidd Chauhan and Shuming Chen. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-01-gaussian-dist"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-01-gaussian-dist"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.32f049b1.js" as="script">
<link rel="preload" href="/APM-2020/main.8b7f41c7.js" as="script">
<link rel="preload" href="/APM-2020/1.e4c79b7e.js" as="script">
<link rel="preload" href="/APM-2020/2.9a4f08f3.js" as="script">
<link rel="preload" href="/APM-2020/37.aad1e583.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.d8642ff2.js" as="script">
<link rel="preload" href="/APM-2020/17896441.5ece2e3d.js" as="script">
<link rel="preload" href="/APM-2020/7261566d.8ab2a8a3.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">🌜</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">🌞</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">The Gaussian Distribution</h1></header><div class="markdown"><p>Authors: <a href="https://www.linkedin.com/in/luke-bravo/" target="_blank" rel="noopener noreferrer">Luke Bravo</a>, <a href="https://www.linkedin.com/in/siddhantchauhan77/" target="_blank" rel="noopener noreferrer">Sidd Chauhan</a> and <a href="https://www.linkedin.com/in/shuming-chen/" target="_blank" rel="noopener noreferrer">Shuming Chen</a>. (<a target="_blank" href="/APM-2020/assets/files/b-01-gaussian-dist-6687aeba553ac80d9b02c6757c4853ca.pdf">PDF</a>)</p><p>The lecture covered the basics of multiple linear regression and the application of maximum likelihood estimation. Fundamental to both of these topics is the basic gaussian distribution, which these notes cover first. Then, we cover maximum likelihood functions and estimations. Next, we look at the equation, assumptions, and limitations of multiple linear regression. Finally, we have included two videos that provide more information on maximum likelihood estimation, and tie these three topics together.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="gaussian-distribution"></a>Gaussian Distribution<a aria-hidden="true" tabindex="-1" class="hash-link" href="#gaussian-distribution" title="Direct link to heading">#</a></h2><p>Gaussian Distribution is interchangeable with normal probability distribution or a ‘bell curve’. It is symmetrical about the mean of its data points, and points closer to the mean are more common than points far off from the mean. A key characteristic of data that have a gaussian or normal distribution is that they are independent and identically distributed (i.i.d.). The i.i.d. assumption is central to machine learning theory; it asserts that data are consistent and uniform by virtue of the way they were sampled.</p><p>In general terms, functions with gaussian distributions are of the following form, where <em>a</em> and <em>b</em> are constants and <em>c</em> is a non-zero constant:</p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/13993a37c117176295fada7cdaa9c1ef1ae769f7"><p>The equation below is one way of writing the probability density for the Gaussian distribution, where <em>mu</em> is the mean, <em>sigma</em> is the standard deviation, and <em>sigma^2</em> is the variance.</p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/61463c3decedda46e356782e24051ec7dd3c34c8"><p>The function has its peak at the mean, and its “spread” increases with the standard deviation (the function reaches 0.607 times its maximum at <em>x + sigma</em> and <em>x - sigma</em>. This implies that normal is more likely to return samples lying close to the mean, rather than those far away. <em>From numpy documentation and P. R. Peebles Jr., “Central Limit Theorem” in “Probability, Random Variables and Random Signal Principles”, 4th ed., 2001, pp. 51, 51, 125.</em></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="transformation-of-variables-to-follow-gaussian-distribution"></a>Transformation Of Variables To Follow Gaussian Distribution:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#transformation-of-variables-to-follow-gaussian-distribution" title="Direct link to heading">#</a></h3><p>These are some direct, mathematical methods to transform variables so that they follow Gaussian distribution. None of them is better than the other. They majorly depend on the original distribution of the variables. They are,</p><ul><li>Logarithmic Transformation</li><li>Reciprocal Transformation</li><li>Square Root Transformation</li><li>Exponential Transformation</li></ul><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="maximum-likelihood-estimation"></a>Maximum Likelihood Estimation<a aria-hidden="true" tabindex="-1" class="hash-link" href="#maximum-likelihood-estimation" title="Direct link to heading">#</a></h2><p>Maximum Likelihood Estimation is a method for estimating parameters of a probability distribution. We saw in class that maximizing a likelihood function (or minimizing its negative in the context of a cost function, see example equation below) lets you calculate the mean and variance of a distribution.</p><p>In most cases, it is best to take the natural log of a likelihood function and then maximize or minimize the result; this is then called log-likelihood. This simplifies multiplication to addition and cancels the e term. You can also avoid differentiating to test for maxima and then differentiating again by doing this. Professor Ghosh explained that we can rest assured that the extrema of the log-likelihood function will correspond to those of the original likelihood function because the logarithm is a monotonically decreasing function.</p><p>The two following equations express the likelihood and log-likelihood functions, respectively:</p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/fa485e7acf98b3a0ce236ce7293f63dd89f84b96"><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/70323a65c0f24cb9b3e9bb0e1a8cf30442c350a7"><p>Note that for the likelihood function,</p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/5a0e1848f54c87072bd9ebf8871ceeb19e8bb1e9"><p>is a product of two single-variable functions.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="multiple-linear-regression"></a>Multiple Linear Regression<a aria-hidden="true" tabindex="-1" class="hash-link" href="#multiple-linear-regression" title="Direct link to heading">#</a></h2><p>A linear regression model is a statistical model that’s frequently used in data science. It’s also one of the basic building blocks of machine learning! Multiple linear regression (MLR/multiple regression) is a statistical technique. It can use several variables to predict the outcome of a different variable. The goal of multiple regression is to model the linear relationship between your independent variables and your dependent variable. It looks at how multiple independent variables are related to a dependent variable.</p><p>Multiple Linear Regression is a statistical technique that models the relationship between a scalar response and one or more explanatory vairables. It is often fitted using the Least Square approach. The general equation for a multiple linear regression is as follows:</p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/704b31aa61dfc93d672f15bf02aa6d168be49643"><p>There are two assumptions behind the model, and they are as follows:</p><ol><li><p>The conditional mean of Y is linear in the Xi variables</p></li><li><p>The error terms \ epsoilon \ are normally distributed, independent from each other, and have constant variance.</p></li></ol><p>The model also has its limitations, one of them being that it does not model the distribution of x.</p><p>MLR is used extensively in econometrics and financial inference.</p><p>Minimizing the mean square error is equivalent to maximizing likelihood under a Gaussian model.</p><p>Least squares estimation finds the line that minuses total squared distance between the actual data point and the regression line. Maximum likelihood estimation maximizes the total probability of the data. Under Gaussian distribution, maximum probability is close to the mean value and this is equivalent to minimizing the distance between the data point and the mean value since Gaussian distribution is symmetric. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="resources"></a>Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#resources" title="Direct link to heading">#</a></h2><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="guassian-distribution"></a>Guassian Distribution<a aria-hidden="true" tabindex="-1" class="hash-link" href="#guassian-distribution" title="Direct link to heading">#</a></h3><p>This <a href="https://medium.com/towards-artificial-intelligence/introduction-to-gaussian-distribution-726be7dc5046" target="_blank" rel="noopener noreferrer">article</a> further explains gaussian distribution.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="maximum-likelihood-estimation-1"></a>Maximum Likelihood Estimation<a aria-hidden="true" tabindex="-1" class="hash-link" href="#maximum-likelihood-estimation-1" title="Direct link to heading">#</a></h3><p>This <a href="https://www.youtube.com/watch?v=XepXtl9YKwc&amp;ab_channel=StatQuestwithJoshStarmer" target="_blank" rel="noopener noreferrer">video</a> gives a brief overview of MLE. The example provided helps better visualize how the estimation works.  </p><p>This <a href="https://towardsdatascience.com/probability-concepts-explained-maximum-likelihood-estimation-c7b4342fdbb1#:~:text=Maximum%20likelihood%20estimation%20is%20a,data%20that%20were%20actually%20observed" target="_blank" rel="noopener noreferrer">article</a> further explains MLE.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="multiple-linear-regression-1"></a>Multiple Linear Regression<a aria-hidden="true" tabindex="-1" class="hash-link" href="#multiple-linear-regression-1" title="Direct link to heading">#</a></h3><p>This <a href="https://towardsdatascience.com/multiple-linear-regression-in-four-lines-of-code-b8ba26192e84" target="_blank" rel="noopener noreferrer">article</a> further explains MLR and how to write the basic code for MLR.</p><p>This <a href="https://www.investopedia.com/terms/m/mlr.asp#:~:text=Key%20Takeaways-,Multiple%20linear%20regression%20(MLR)%2C%20also%20known%20simply%20as%20multiple,uses%20just%20one%20explanatory%20variable" target="_blank" rel="noopener noreferrer">artilce</a> explains how to apply MLR in investing and finance contexts.</p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-22T00:55:23.000Z" class="docLastUpdatedAt_217_">10/21/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-a/a-14-pca"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">« Principal Components Analysis</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-02-mlr"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Multiple Linear Regression »</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#gaussian-distribution" class="table-of-contents__link">Gaussian Distribution</a><ul><li><a href="#transformation-of-variables-to-follow-gaussian-distribution" class="table-of-contents__link">Transformation Of Variables To Follow Gaussian Distribution:</a></li></ul></li><li><a href="#maximum-likelihood-estimation" class="table-of-contents__link">Maximum Likelihood Estimation</a></li><li><a href="#multiple-linear-regression" class="table-of-contents__link">Multiple Linear Regression</a></li><li><a href="#resources" class="table-of-contents__link">Resources</a><ul><li><a href="#guassian-distribution" class="table-of-contents__link">Guassian Distribution</a></li><li><a href="#maximum-likelihood-estimation-1" class="table-of-contents__link">Maximum Likelihood Estimation</a></li><li><a href="#multiple-linear-regression-1" class="table-of-contents__link">Multiple Linear Regression</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright © 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.32f049b1.js"></script>
<script src="/APM-2020/main.8b7f41c7.js"></script>
<script src="/APM-2020/1.e4c79b7e.js"></script>
<script src="/APM-2020/2.9a4f08f3.js"></script>
<script src="/APM-2020/37.aad1e583.js"></script>
<script src="/APM-2020/f976f453.d8642ff2.js"></script>
<script src="/APM-2020/17896441.5ece2e3d.js"></script>
<script src="/APM-2020/7261566d.8ab2a8a3.js"></script>
</body>
</html>