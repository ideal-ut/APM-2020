<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Misclassification Rate and Minimizing Expected Loss | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Misclassification Rate and Minimizing Expected Loss | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Kaiwen Zhang, Christina Zhao, Qinwen Zhou. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Kaiwen Zhang, Christina Zhao, Qinwen Zhou. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-16-loss"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-16-loss"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.ee71a697.js" as="script">
<link rel="preload" href="/APM-2020/main.34e32ce5.js" as="script">
<link rel="preload" href="/APM-2020/1.433d51c7.js" as="script">
<link rel="preload" href="/APM-2020/2.8e74170b.js" as="script">
<link rel="preload" href="/APM-2020/38.a3af94e3.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.4a8df433.js" as="script">
<link rel="preload" href="/APM-2020/17896441.b66b5b0d.js" as="script">
<link rel="preload" href="/APM-2020/54e09886.22ccf965.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-16-loss">Misclassification Rate and Minimizing Expected Loss</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Misclassification Rate and Minimizing Expected Loss</h1></header><div class="markdown"><p>Authors: Kaiwen Zhang, Christina Zhao, Qinwen Zhou. (<a target="_blank" href="/APM-2020/assets/files/b-16-loss-f410f2d37659441b656aff27118189d1.pdf">PDF</a>)</p><p>This lecture went over the Misclassification Rate for the Bayes Classifier and how to minimize the expected loss. The Misclassification Rate is the probability that one observation (an instance) is mislabeled by the classifier- essentially the accuracy of the model. The Expected Loss is the cost of a particular outcome. When minimizing the expected loss of the Bayes Classifier, thresholds are selected that the class label assigned an instance will minimize the error. In a symmetric loss function, all errors are equally costly, whereas in an asymmetric loss function, different errors have different losses. Thus, the choice of threshold is to increase the probability of the low cost outcomes (minimizing the expected loss). Minimizing loss may not necessarily mean that the Misclassification rate is minimized, so the boundary choice is a trade-off.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bayes-theorem-and-its-applications-in-machine-learning"></a>Bayes Theorem and its applications in Machine Learning<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bayes-theorem-and-its-applications-in-machine-learning" title="Direct link to heading">#</a></h2><i>The notes below referenced Bayes Theorem topic from Machine Learning Mastery.</i><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bayes-theorem"></a>Bayes Theorem:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bayes-theorem" title="Direct link to heading">#</a></h3><p>Principled way of calculating a conditional probability without the joint probability.   </p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">âˆ£</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">âˆ£</mi><mi>A</mi><mo stretchy="false">)</mo><mo>âˆ—</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(A|B) = P(B|A) * P(A) / P(B)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mord">âˆ£</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mord">âˆ£</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">A</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.05017em">B</span><span class="mclose">)</span></span></span></span></span></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="maximum-a-posteriorimap"></a>Maximum a Posteriori(MAP)<a aria-hidden="true" tabindex="-1" class="hash-link" href="#maximum-a-posteriorimap" title="Direct link to heading">#</a></h3><p>Maximum a posteriori is a use of Bayes Theorem for modeling hypotheses. It is defined as the optimization or seeking the hypothesis with the maximum posterior probability (note: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>h</mi><mi mathvariant="normal">âˆ£</mi><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(h|D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord">âˆ£</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">)</span></span></span></span></span> in <i>eq. 1</i>). </p><b> Hypothesis:</b>Fitting different machine learning algorithms or models on the data to map inputs to outputs can be thought of as testing several hypotheses about the relationships in data. For example, based on the width and length of petals, we can predict the categories of iris flowers. More on Hypothesis in Machine Learning.<p>Bayes Theorem provides a probabilistic framework to describe the relationship between data and a hypothesis.  </p><p><em>eq 1</em>:
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>h</mi><mi mathvariant="normal">âˆ£</mi><mi>D</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mi mathvariant="normal">âˆ£</mi><mi>h</mi><mo stretchy="false">)</mo><mo>âˆ—</mo><mi>P</mi><mo stretchy="false">(</mo><mi>h</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>P</mi><mo stretchy="false">(</mo><mi>D</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h|D) = P(D|h) * P(h) / P(D)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mord">âˆ£</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mord">âˆ£</span><span class="mord mathnormal">h</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">h</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02778em">D</span><span class="mclose">)</span></span></span></span></span>   </p><p><em>h stands for hypothesis, D stands for data</em>  </p><p>Using MAP can help us find the hypothesis that best describes the observed data. It provides a Bayesian foundation for common machine learning algorithms like linear regression. Another probabilistic framework that achieves the same goal is Maximum Likelihood Estimation(MLE). Further reading on the differences between MLE and MAP can be found at <a href="https://machinelearningmastery.com/maximum-a-posteriori-estimation/" target="_blank" rel="noopener noreferrer">https://machinelearningmastery.com/maximum-a-posteriori-estimation/</a>.  </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="bayes-optimal-classifier-and-naive-bayes-classifier"></a>Bayes Optimal Classifier and Naive Bayes Classifier<a aria-hidden="true" tabindex="-1" class="hash-link" href="#bayes-optimal-classifier-and-naive-bayes-classifier" title="Direct link to heading">#</a></h3><p>Both classifiers are uses of Bayes Theorem for classification. The difference between Bayes Optimal Classifier and MAP is that MAP framework seeks the most probable model while Bayes Optimal Classifier looks for the best classification of the new instance.   </p><p>In real-life, Bayes Classifier is computationally expensive especially when the number of input variables increases. In class, the example we are shown was that given the volume of a fruit observed, predict whether it is an apple or orange, which is quite straightforward as we were only considering one feature. However, computing the posterior probability of real data is not feasible as it is hard to get the conditional probability of the observation(which could be represented by 30+ features) based on the class.   </p><p><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>X</mi><mn>1</mn><mo separator="true">,</mo><mi>X</mi><mn>2</mn><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mo separator="true">,</mo><mi>X</mi><mi>n</mi><mi mathvariant="normal">âˆ£</mi><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo><mo>âˆ—</mo><mi>P</mi><mo stretchy="false">(</mo><mi>c</mi><mi>l</mi><mi>a</mi><mi>s</mi><mi>s</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mi>P</mi><mo stretchy="false">(</mo><mi>o</mi><mi>b</mi><mi>s</mi><mi>e</mi><mi>r</mi><mi>v</mi><mi>a</mi><mi>t</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">P(X1,X2,...,Xn|class)* P(class)/P(observation)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">1</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord">2</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="mord mathnormal">n</span><span class="mord">âˆ£</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">c</span><span class="mord mathnormal" style="margin-right:0.01968em">l</span><span class="mord mathnormal">a</span><span class="mord mathnormal">s</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mord">/</span><span class="mord mathnormal" style="margin-right:0.13889em">P</span><span class="mopen">(</span><span class="mord mathnormal">o</span><span class="mord mathnormal">b</span><span class="mord mathnormal">s</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.02778em">r</span><span class="mord mathnormal" style="margin-right:0.03588em">v</span><span class="mord mathnormal">a</span><span class="mord mathnormal">t</span><span class="mord mathnormal">i</span><span class="mord mathnormal">o</span><span class="mord mathnormal">n</span><span class="mclose">)</span></span></span></span></span>  </p><p>Naive Bayes Classifier simplifies the calculation by making the assumption that <b>each input variable is independent from each other.</b> Hence the calculation becomes
<img src="https://i.imgur.com/XKjEi0p.jpg" alt="alt text"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="misclassification-rate"></a>Misclassification Rate<a aria-hidden="true" tabindex="-1" class="hash-link" href="#misclassification-rate" title="Direct link to heading">#</a></h2><p>Resource: <i>Pattern Recognition and Machine Learning, Christopher M. Bishop</i></p><p>The Misclassification Rate is the probability of an instance being assigned the incorrect class label by the classifier. To start, we need a rule that will divide the input space (for the observations) into Decision Regions (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">R_k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>), one for each class (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>), where if a given value of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> is in the region, it will be assigned the label of class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>. </p><p>As an example, we can look at a 2 class problem, with a Class 1 (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">C_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>) and Class 2. (<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">C_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>)</p><p><img src="https://i.imgur.com/BMrQpD9.jpg" alt="alt text"></p><p>The probability of making a misclassification is denoted as the sum of the integral of the probability of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mn>1</mn></msub></mrow><annotation encoding="application/x-tex">R_1</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> belonging to Class 2 (the red and the green areas under the curve) and the probability of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> in <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">R_2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> belonging to Class 1(the blue area under the curve).</p><p><img src="https://i.imgur.com/RDgMgnN.jpg" alt="alt text"></p><p>In order to <b>minimize</b> the probability of a mistake occuring ( <i>p(mistake)</i> ), the decision rule should be chosen so that <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> is assigned to the class with an area under the curve with the smallest value (the smallest loss) at the point <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>.
For example, if  <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mn>1</mn></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_1, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> &lt; <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mn>2</mn></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_2, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>, then <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> should be assigned to Class 1.</p><p>Another way of looking at how to minimize the probability of a mistake occuring is in terms of <b>posterior probability</b> of a class. </p><p>Using the product rule of probability (the probability of two independent events occurring together can be found by multiplying the individual probabilities of each event occurring alone), we get <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant="normal">âˆ£</mi><mi>x</mi><mo stretchy="false">)</mo><mo>âˆ—</mo><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_k, x) = p(C_k|x) * p(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>: that the probability of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> being in the class is equal to the posterior probability multiplied by the probability of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> occurring on its own. </p><p>Since:
<img src="https://i.imgur.com/0a4XvJ1.jpg" alt="alt text">
we can ignore <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>, as it is common to both equations. We can then think of it as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_k, x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> =  <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant="normal">âˆ£</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_k|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span>, where misclassification can be minimized by choosing to assign <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> to the class where the posterior probability <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant="normal">âˆ£</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_k|x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> is the largest.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="minimizing-expected-loss"></a>Minimizing Expected Loss<a aria-hidden="true" tabindex="-1" class="hash-link" href="#minimizing-expected-loss" title="Direct link to heading">#</a></h2><p>Loss matrix is introduced to encode a penalty for misclassification errors. Sometimes we will see a negative entry in the loss matrix indicating a reward for making the correct prediction.</p><p>A loss matrix is introduced to encode a penalty for misclassification errors for any given combination of true class and classified class. The convention is that a penalty is indicated by positive values; however, sometimes we will see negative entries in the loss matrix which indicate a reward for making the correct classification (same true and classified class).</p><p>The mathematical expression for the expected loss is the following equation:</p><p><img src="https://i.imgur.com/TQSWe5g.jpg"></p><p>where a <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> represents a true class and a <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> represents a classified class. <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">R_j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.969438em;vertical-align:-0.286108em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.00773em">R</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em"><span style="top:-2.5500000000000003em;margin-left:-0.00773em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em">j</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em"><span></span></span></span></span></span></span></span></span></span></span> is the decision region on the <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> domain for classifying a class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>. The equation illustrates the following process: the product of a penalty (or sometimes reward) value and the joint probability of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> being of a particular true class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span> should be evaluated and aggregated across all values of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> that are to be classified as class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>, do that across all the different classified states, then more broadly across all the different true states.</p><p>Intuitively, if we treat the penalty/reward value as a constant, the focus of the calculus becomes <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x, C_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> integrated across the decision region for a certain class <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span>, and this can be seen as the probability for classifying <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.85396em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.05724em">j</span></span></span></span></span> while actually <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal" style="margin-right:0.03148em">k</span></span></span></span></span>. The two summations to the left of the integration, therefore, are just the summing up of the products of penalty/reward value and probability of true-classified-states combinations, across all true-classified state combinations.</p><p><img src="https://i.imgur.com/zaEgJ2A.jpg"></p><p>The matrix above shows the penalties for (mis-)detecting cancer. The expected loss in this case will be <b>0*p(detect cancer &amp; true cancer) + 1000*p(detecting normal &amp; true cancer) + 1*p(detecting cancer &amp; true normal) + 0*p(detecting normal and true normal)</b>. This example is an application of the logic described in the previous paragraph.</p><p>However, since <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msub><mi>C</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x, C_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span></span> is essentially the same as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>C</mi><mi>k</mi></msub><mi mathvariant="normal">âˆ£</mi><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(C_k | x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em">k</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">âˆ£</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> because of the presence of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span></span> common to all terms, we can establish the expected-loss optimization as follows:</p><p><img src="https://i.imgur.com/OGTutoD.jpg">  </p><p>Translated into English, and returning to the cancer example, the optimization story is as follows: For each column (classified class), sum the products of the penalties and posterior probabilities of true classes. For example, if <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> is such that <b><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal">p</span></span></span></span></span>(<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{cancer}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> | <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>) = 0.4</b> and <b><span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal">p</span></span></span></span></span>(<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>n</mi><mi>o</mi><mi>r</mi><mi>m</mi><mi>a</mi><mi>l</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{normal}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">o</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span><span class="mord mathnormal mtight">m</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight" style="margin-right:0.01968em">l</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> | <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>) = 1 - <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal">p</span></span></span></span></span>(<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mrow><mi>c</mi><mi>a</mi><mi>n</mi><mi>c</mi><mi>e</mi><mi>r</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_{cancer}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07153em">C</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:-0.07153em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">a</span><span class="mord mathnormal mtight">n</span><span class="mord mathnormal mtight">c</span><span class="mord mathnormal mtight">e</span><span class="mord mathnormal mtight" style="margin-right:0.02778em">r</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span> | <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>) = 0.6</b>, then the sum of the products for the â€œcancerâ€ column would be the â€œexpected valueâ€ 0*0.4+1*0.6=0.6 and for the â€œnormalâ€ column the expected value would be 1000*0.4+0*0.6=400.<br>
The optimal choice here is the classification that minimizes loss for this particular <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span> -- cancer in this case. Doing this for all values of <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>, all <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>â€™s with the same smaller-expected-value column being â€œcancerâ€ will form a decision region, and all <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em"></span><span class="mord mathnormal">x</span></span></span></span></span>â€™s having â€œnormalâ€ being the smaller expected-value producer will form the other decision region. These two decision regions constitute the optimal rules for minimizing expected loss. Of course, this process can be generalized to more than two classes/regions as well.</p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-11-04T11:20:11.000Z" class="docLastUpdatedAt_217_">11/4/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-15-dt"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Classification - Decision Trees and Bayes Decision Theory</div></a></div><div class="pagination-nav__item pagination-nav__item--next"></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#bayes-theorem-and-its-applications-in-machine-learning" class="table-of-contents__link">Bayes Theorem and its applications in Machine Learning</a><ul><li><a href="#bayes-theorem" class="table-of-contents__link">Bayes Theorem:</a></li><li><a href="#maximum-a-posteriorimap" class="table-of-contents__link">Maximum a Posteriori(MAP)</a></li><li><a href="#bayes-optimal-classifier-and-naive-bayes-classifier" class="table-of-contents__link">Bayes Optimal Classifier and Naive Bayes Classifier</a></li></ul></li><li><a href="#misclassification-rate" class="table-of-contents__link">Misclassification Rate</a></li><li><a href="#minimizing-expected-loss" class="table-of-contents__link">Minimizing Expected Loss</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.ee71a697.js"></script>
<script src="/APM-2020/main.34e32ce5.js"></script>
<script src="/APM-2020/1.433d51c7.js"></script>
<script src="/APM-2020/2.8e74170b.js"></script>
<script src="/APM-2020/38.a3af94e3.js"></script>
<script src="/APM-2020/f976f453.4a8df433.js"></script>
<script src="/APM-2020/17896441.b66b5b0d.js"></script>
<script src="/APM-2020/54e09886.22ccf965.js"></script>
</body>
</html>