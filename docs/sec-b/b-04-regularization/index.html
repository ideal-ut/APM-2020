<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Regularization | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Regularization | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Mervyn Giritharan, Calie Gilmore, Martina Galvan. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Mervyn Giritharan, Calie Gilmore, Martina Galvan. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-04-regularization"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-04-regularization"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2cffed3b.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.1402fe68.js" as="script">
<link rel="preload" href="/APM-2020/main.2d013a91.js" as="script">
<link rel="preload" href="/APM-2020/1.1bd411a9.js" as="script">
<link rel="preload" href="/APM-2020/2.a2e25927.js" as="script">
<link rel="preload" href="/APM-2020/32.9618ee48.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.84bb295e.js" as="script">
<link rel="preload" href="/APM-2020/17896441.32dbdef4.js" as="script">
<link rel="preload" href="/APM-2020/f824b889.dcfb1cf0.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Regularization</h1></header><div class="markdown"><p>Authors: Mervyn Giritharan, Calie Gilmore, Martina Galvan. (<a target="_blank" href="/APM-2020/assets/files/b-04-regularization-6c0b89c0692db51b59482204ae650ac7.pdf">PDF</a>)</p><p>This lecture covers the benefits of regularization to avoid overfitting the testing data set.</p><p>Regularization reduces the weights to fit the test set better, however a penalty is added. There can be many possible options to use regularization, such as Lasso and Ridge, so you need to use domain knowledge to decide which regularization method is preferred to constrain your solution.
Ultimately, the goal for regularization is to avoid over-fitting to result in better predictions. When a model is over fitted, it&#x27;s hyper adjusting to past data that doesn&#x27;t allow us to accurately predict the future data. </p><p>The regularization process imposes a penalty.
The cost of the penalty: Cost = MSE +  <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal">Î»</span></span></span></span></span> Penalty(f)
In this formula, the MSE is the mean squared error which is the average square of the errors. Lambda is the regularization parameter and we want to find the mabda that results in the lowest variance on the test set. The regularization penalty is a function that maps each function f onto a number. Note that the penalty is not applied to the y-intercept.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="ridge-regression"></a>Ridge Regression<a aria-hidden="true" tabindex="-1" class="hash-link" href="#ridge-regression" title="Direct link to heading">#</a></h2><p>In Ridge regression, the regression is the least squares regression with the ridge regression penalty added to it.</p><p>The penalty in the cost function is the sum of the squared weights/coefficients.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><msubsup><mi>W</mi><mi>J</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">p=\sum \limits_{i=1}^{M} W_{J}^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.506005em;vertical-align:-0.9776689999999999em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283360000000004em"><span style="top:-2.122331em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em"><span class="pstrut" style="height:3em"></span><span><span class="mop op-symbol small-op">âˆ‘</span></span></span><span style="top:-3.950005em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.9776689999999999em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em"><span style="top:-2.424669em;margin-left:-0.13889em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em">J</span></span></span></span><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em"><span></span></span></span></span></span></span></span></span></span></span></p><p>The lambda penalty regaularizes the coefficients and penalizes them if they are too large. The is the formula for the penalty being added to the error.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mo stretchy="false">(</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></msubsup><mo stretchy="false">(</mo><mrow><msup><mi>w</mi><mi>T</mi></msup><mi>Ï•</mi><mo stretchy="false">(</mo><msub><mi>X</mi><mi>n</mi></msub><mo stretchy="false">)</mo><mo>âˆ’</mo><msub><mi>t</mi><mi>n</mi></msub></mrow><msup><mo stretchy="false">)</mo><mn>2</mn></msup><mo>+</mo><mfrac><mi>Î»</mi><mn>2</mn></mfrac><mi mathvariant="normal">âˆ£</mi><mi mathvariant="normal">âˆ£</mi><mi>w</mi><mi mathvariant="normal">âˆ£</mi><msup><mi mathvariant="normal">âˆ£</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">E(w)=\sum \limits_{n=1}^{N}({w^T \phi(X_{n}) - t_{n}})^2 + \frac{\lambda}{2} ||w||^2</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.05764em">E</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.4954490000000003em;vertical-align:-0.9671129999999999em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283360000000004em"><span style="top:-2.132887em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em"><span class="pstrut" style="height:3em"></span><span><span class="mop op-symbol small-op">âˆ‘</span></span></span><span style="top:-3.950005em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">N</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.9671129999999999em"><span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em">T</span></span></span></span></span></span></span></span><span class="mord mathnormal">Ï•</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.07847em">X</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:-0.07847em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ’</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mord"><span class="mord mathnormal">t</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">n</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="mclose">)</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1.2251079999999999em;vertical-align:-0.345em"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8801079999999999em"><span style="top:-2.6550000000000002em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">2</span></span></span></span><span style="top:-3.23em"><span class="pstrut" style="height:3em"></span><span class="frac-line" style="border-bottom-width:0.04em"></span></span><span style="top:-3.394em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">Î»</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mord">âˆ£</span><span class="mord">âˆ£</span><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="mord">âˆ£</span><span class="mord"><span class="mord">âˆ£</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em"><span style="top:-3.063em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span></span></span></span></span></p><p>This can be called shrinkage (stats) or weight decay (neaural nets). The regularization coefficient <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î»</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em"></span><span class="mord mathnormal">Î»</span></span></span></span></span> now controls the effective model complexity, therefore reducing the coefficients helps decrease model complexity. Ridge is also good because it stabilizes answers and helps adjust for collinearity. The penalty added by the ridge regression creates the bias needed on the training set in order to reduce the variance on the test set and have more accurate predictions in the long run.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="lasso-regression"></a>Lasso Regression<a aria-hidden="true" tabindex="-1" class="hash-link" href="#lasso-regression" title="Direct link to heading">#</a></h2><p>Lasso regression is super similar to ridge regression, but the one difference is the ridge regression squares the variable weights for the penalty, whereas the lasso regression takes the absolute value of the variable weights for the penalty.</p><p>In Lasso regression, the penalty in the cost function uses the sum of the absolute values of weights.
<span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>M</mi></msubsup><mi mathvariant="normal">âˆ£</mi><msub><mi>W</mi><mi>J</mi></msub><mi mathvariant="normal">âˆ£</mi></mrow><annotation encoding="application/x-tex">p=\sum \limits_{i=1}^{M} |W_{J}|</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.506005em;vertical-align:-0.9776689999999999em"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5283360000000004em"><span style="top:-2.122331em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">i</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0000050000000003em"><span class="pstrut" style="height:3em"></span><span><span class="mop op-symbol small-op">âˆ‘</span></span></span><span style="top:-3.950005em;margin-left:0em"><span class="pstrut" style="height:3em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em">M</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.9776689999999999em"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord">âˆ£</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.13889em">W</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em"><span style="top:-2.5500000000000003em;margin-left:-0.13889em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.09618em">J</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mord">âˆ£</span></span></span></span></span></p><p>A second difference is that lasso regression takes magnitudes into account which means the increase in lambda can cause some coefficients to zero out. This helps us know which features can be removed from the model, as well as reduce the over-fitting. The ridge regression cannot zero out features because it can only shrink the slope asymptotically toward zero. So lasso regression is a little better than ridge regression in reducing the variance when there are many insignificant variables. However, ridge regression is a little better when many of the variables are proven useful.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="when-to-use-lasso-vs-ridge"></a>When to Use Lasso vs. Ridge?<a aria-hidden="true" tabindex="-1" class="hash-link" href="#when-to-use-lasso-vs-ridge" title="Direct link to heading">#</a></h2><p>One way is to use both methods and see which one provides optimal results. However, that is an expensive option and often, you cannot perform both. So, it is good to know when each method is the optimal solution. There are some considerations to keep in mind: you want your model to be only as complicated as necessary but not too complicated, and it is important to keep in mind how much data you have. See the Data Set Size section below. It is also important to standardize your data prior to using Lasso and Ridge to compare the weights.</p><p>Lasso will help you determine which variables you want to ignore or drop from your model. It helps you to do this by more aggressively pushing your lower weights to 0. So, if you have a model will 10 different weights and you know that they are all useful, then lasso may not be the optimal method. However, if you have a model with 10 weights and you are not sure if they are useful then lasso would help in determining this. Lasso is more aggressive on weights that &lt; 1.</p><p>Ridge more aggressively contains the higher weights, especially weights that are &lt; 1. The Ridge regression discourages larger values by adding the penalty term to error. Here, the regularization coefficient, lambda, controls the effective model complexity. Ridge can be a good method to use because it stabilizes the answers and helps with a collinearity problem. Why is this? If 2 variables are collinear, the determinant of the matrix is small, and the inverse of the matrix is large. This results in your weights varying a lot. When some variables are collinear, the lambda is going to lessen the uncertainty that is associated with the determinant of the matrix being small and the inverse therefore being very large. This results in a more stable answer and contains less variance.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="data-set-size"></a>Data Set Size<a aria-hidden="true" tabindex="-1" class="hash-link" href="#data-set-size" title="Direct link to heading">#</a></h2><p><img src="https://imgur.com/TC6kylr.jpg"></p><p>Regularization will help the N=15 chart more because it doesn&#x27;t have a lot of data. It&#x27;s harder to fit a model with limited size data sets and it can be more likely to overfit. Regularization allows complex models to be trained with small data sets without severely over-fitting because it adds a penalty to the model. The penalty introduces bias into model to combat overfitting and therefore reduces the variance in the test set. In contrast, regularization does not help the N=100 chart very much because it has a good amount of data and is less likely to need the penalty to combat overfitting. </p><p>Another note on data set size is that usually it&#x27;s best practice to have at least as many data points as you have parameters. However, ridge regression can remove the requirement and allows you to have more parameters that data points. This is very useful in the practical world where producing very large data sets can be challenging.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="more-resources"></a>More Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#more-resources" title="Direct link to heading">#</a></h2><p>Ridge &amp; Lasso Regularization Info: <a href="https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/ridge-and-lasso-regression-a-complete-guide-with-python-scikit-learn-e20e34bcbf0b</a></p><p>Regularization Process &amp; Benefits: <a href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf" target="_blank" rel="noopener noreferrer">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p><p>Video on Regularization using Ridge Regression: <a href="https://www.youtube.com/watch?v=Q81RR3yKn30" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=Q81RR3yKn30</a></p><p>Video on Regularization Difference between Ridge and Lasso Regressions: <a href="https://www.youtube.com/watch?v=NGf0voTMlcs&amp;t=288s" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=NGf0voTMlcs&amp;t=288s</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-22T00:55:23.000Z" class="docLastUpdatedAt_217_">10/21/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-03-parametric-models"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Function approximation (Regression)</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-05-bias-variance"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Bias-Variance Dilemma Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#ridge-regression" class="table-of-contents__link">Ridge Regression</a></li><li><a href="#lasso-regression" class="table-of-contents__link">Lasso Regression</a></li><li><a href="#when-to-use-lasso-vs-ridge" class="table-of-contents__link">When to Use Lasso vs. Ridge?</a></li><li><a href="#data-set-size" class="table-of-contents__link">Data Set Size</a></li><li><a href="#more-resources" class="table-of-contents__link">More Resources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2cffed3b.js"></script>
<script src="/APM-2020/runtime~main.1402fe68.js"></script>
<script src="/APM-2020/main.2d013a91.js"></script>
<script src="/APM-2020/1.1bd411a9.js"></script>
<script src="/APM-2020/2.a2e25927.js"></script>
<script src="/APM-2020/32.9618ee48.js"></script>
<script src="/APM-2020/f976f453.84bb295e.js"></script>
<script src="/APM-2020/17896441.32dbdef4.js"></script>
<script src="/APM-2020/f824b889.dcfb1cf0.js"></script>
</body>
</html>