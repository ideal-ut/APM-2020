<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Neural Networks | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Neural Networks | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Matthew Leong, Qianhui Lian, India Lindsay. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Matthew Leong, Qianhui Lian, India Lindsay. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-08-nn"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-08-nn"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2cffed3b.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.1402fe68.js" as="script">
<link rel="preload" href="/APM-2020/main.2d013a91.js" as="script">
<link rel="preload" href="/APM-2020/1.1bd411a9.js" as="script">
<link rel="preload" href="/APM-2020/2.a2e25927.js" as="script">
<link rel="preload" href="/APM-2020/32.9618ee48.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.84bb295e.js" as="script">
<link rel="preload" href="/APM-2020/17896441.32dbdef4.js" as="script">
<link rel="preload" href="/APM-2020/f1296005.13349ef3.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Neural Networks</h1></header><div class="markdown"><p>Authors: Matthew Leong, Qianhui Lian, India Lindsay. (<a target="_blank" href="/APM-2020/assets/files/b-08-nn-89bde8530d1f651ed4eec14b277c9852.pdf">PDF</a>)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="summary-of-lecture"></a>Summary of Lecture<a aria-hidden="true" tabindex="-1" class="hash-link" href="#summary-of-lecture" title="Direct link to heading">#</a></h2><p>In this lecture, we were introduced to the concept of neural networks. We walked through the similarities between an actual neuron and a neural network. We discussed the very first neural network model, the adaptive linear element. We then focused on how neural networks operate. We finalized by touching on the linear transformations that would occur in a network of linear neurons (neurons which have an identity activation function). In this lecture supplement, we touch on and expand upon each of these four areas.  </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="analogy-of-a-neuron"></a>Analogy of a Neuron:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#analogy-of-a-neuron" title="Direct link to heading">#</a></h3><p>A neural network is a machine learning approach that models the human brain. In the below image, you can observe the comparison of elements between a neuron, a simple neural net, and a neural net with 2 hidden layers. </p><p>The &quot;synapse&quot; of a neural net is where the input variables are received.</p><p>The &quot;dendrite&quot; is where the weights are computed for each input variable. </p><p>The &quot;cell body&quot; is where all the weights are summed up. </p><p>At the start of the &quot;axon&quot;, the nonlinear transformation is applied. The &quot;axon&quot; then carries the resulting output to the &quot;axon terminal&quot;. </p><p>The &quot;axon terminal&quot; represents the output of the neural net. </p><p><img src="/APM-2020/assets/images/neuron-0eef89b785ce57ac7014c1b3e1a3f0c9.jpg"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="history-of-adaptive-linear-elements-qianhui"></a>History of Adaptive Linear Elements: Qianhui<a aria-hidden="true" tabindex="-1" class="hash-link" href="#history-of-adaptive-linear-elements-qianhui" title="Direct link to heading">#</a></h3><p>As Dr. Ghosh mentioned in class, ADALINE (ADAptive LINear Element) was created by Bernard Widrow and Ted Hoff in 1959. ADALINE was inspired in attenpts to model the structures of a neuron. We can look at the model without non-linearity as well as tranforming the linear output to a binary output. The linear output would go to the activation function, that transforms the output. </p><p>Note: The transformation by a binary activation function shown below represents whether the neuro is firing or not. Remember the activation funciton can be non-linear functions other than binary step. </p><p>Here is a break down of the model works:</p><p>The model takes a set of inputs <em>x</em> then multipled them to a set of weights <em>w</em> (Note: input(x0) for the bias weight(w0) is always positive 1). These products are added together to generate a weighted sum, we will call this <em>y</em> (our output using the given input). To train the model, <em>y</em> is comparied to the desired output for the specific input. The difference between them is generates an error value. The model uses this error to adapt the weights. Through this step, the model adjust the weights and find a linear output with a minimized MSE.</p><p>This linear output could then transform through an activation function (the image below shows a binary function) to generate an output of 1 or -1.</p><p><img src="/APM-2020/assets/images/Adaline-ae861365ae6c56ea863818036b7f2187.png"></p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="neural-networks"></a>Neural Networks<a aria-hidden="true" tabindex="-1" class="hash-link" href="#neural-networks" title="Direct link to heading">#</a></h3><p>Neural networks are comprised of a D dimensional input layer, an M dimensional hidden layer, and a K dimensional output layer. After the inputs are received, the weights are summed up and then passed through a nonlinear activation function, typically a hyperbolic or sigmoid function. This function serves as a universal approximator. When fed any continuous function and hidden layers, a universal approximator will map the function exactly. In this context, it maps it from D dimensional space to K dimensional space. After the nonlinear activation layer, the neural network again sums up the weights and outputs predictions. </p><p>The universal approximator indicates that a mapping can exist from our inputs to our prediction outputs. However, there are still several design questions that serve as parameters in the neural network: the number of hidden units/nodes, # of epochs/iterations of SGD, the type of activation function, and the learning rate used by SGD. </p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="linear-transformations"></a>Linear Transformations<a aria-hidden="true" tabindex="-1" class="hash-link" href="#linear-transformations" title="Direct link to heading">#</a></h3><p>In lecture, we touched on the concept of linear transformations. Linear transformations occur within a linear neural network when the activation function is merely the identity activation function. </p><p> We&#x27;d like to elaborate on this concept and some others to help those without a math background. For instance, why do we bother with vector notation and all the linear/matrix algebra? As mentioned earlier, neural networks have input nodes. How do we repesent all these inputs? Well we can write it out as <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>3</mn></msub><mo separator="true">,</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi><msub><mi>x</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">x_1,x_2,x_3,...x_n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em"></span><span class="mord">.</span><span class="mord">.</span><span class="mord">.</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span>. However, this is quite cumbersome to write out every single time so we can represent it as a vector instead. </p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>x</mi><mo>âƒ—</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>2</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>3</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">.</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">.</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mi>n</mi></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\vec{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \\ . \\ . \\ x_n \\ \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;vertical-align:0em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:7.20703em;vertical-align:-3.3500499999999995em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.8569800000000005em"><span style="top:-0.44997000000000076em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ£</span></span></span><span style="top:-1.5999700000000008em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-2.195970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-2.791970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-3.3879700000000006em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-3.9839700000000007em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-4.579970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-4.615960000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-5.856980000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¡</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:3.3500499999999995em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.850000000000001em"><span style="top:-6.010000000000001em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-4.810000000000001em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.6100000000000003em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">.</span></span></span><span style="top:-1.2100000000000002em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">.</span></span></span><span style="top:-0.009999999999999953em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:3.35em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.8569800000000005em"><span style="top:-0.44997000000000076em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¦</span></span></span><span style="top:-1.5999700000000008em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-2.195970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-2.791970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-3.3879700000000006em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-3.9839700000000007em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-4.579970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-4.615960000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-5.856980000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¤</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:3.3500499999999995em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>Now we can represent our inputs simply as just <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>x</mi><mo>âƒ—</mo></mover></mrow><annotation encoding="application/x-tex">\vec{x}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;vertical-align:0em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span></span></span></span></span></span></span></span>. This keeps the math simple and allows us to do various operations on the vector. For instance, we can do something as simple as multiplying by a number like 2. This would essentially be:</p><hr><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mn>2</mn><mo>âˆ—</mo><mover accent="true"><mi>x</mi><mo>âƒ—</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>x</mi><mn>1</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>x</mi><mn>2</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>x</mi><mn>3</mn></msub></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">.</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi mathvariant="normal">.</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mn>2</mn><msub><mi>x</mi><mi>n</mi></msub></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">2*\vec{x} = \begin{bmatrix} 2x_1 \\ 2x_2 \\ 2x_3 \\ . \\ . \\ 2x_n \\ \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em"></span><span class="mord">2</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">âˆ—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:0.714em;vertical-align:0em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:7.20703em;vertical-align:-3.3500499999999995em"></span><span class="minner"><span class="mopen"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.8569800000000005em"><span style="top:-0.44997000000000076em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ£</span></span></span><span style="top:-1.5999700000000008em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-2.195970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-2.791970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-3.3879700000000006em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-3.9839700000000007em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-4.579970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-4.615960000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¢</span></span></span><span style="top:-5.856980000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¡</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:3.3500499999999995em"><span></span></span></span></span></span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.850000000000001em"><span style="top:-6.010000000000001em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-4.810000000000001em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-3.6100000000000003em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">3</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.41em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">.</span></span></span><span style="top:-1.2100000000000002em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">.</span></span></span><span style="top:-0.009999999999999953em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord">2</span><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.151392em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">n</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:3.35em"><span></span></span></span></span></span></span></span><span class="mclose"><span class="delimsizing mult"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:3.8569800000000005em"><span style="top:-0.44997000000000076em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¦</span></span></span><span style="top:-1.5999700000000008em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-2.195970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-2.791970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-3.3879700000000006em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-3.9839700000000007em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-4.579970000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-4.615960000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¥</span></span></span><span style="top:-5.856980000000001em"><span class="pstrut" style="height:3.1550000000000002em"></span><span class="delimsizinginner delim-size4"><span>âŽ¤</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:3.3500499999999995em"><span></span></span></span></span></span></span></span></span></span></span></span></div><p>This is pretty simple but you might be surprised to learn that it&#x27;s a linear transformation. More specifically it is a scalar transformation. There is also a visual element towards linear transformations. However, it is hard to visualize n-dimensions so let&#x27;s consider a 2-d vector which can be represented as:</p><div class="math math-display"><span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>x</mi><mo>âƒ—</mo></mover><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.15999999999999992em" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>1</mn></msub></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msub><mi>x</mi><mn>2</mn></msub></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\vec{x} = \begin{bmatrix} x_1 \\ x_2 \end{bmatrix}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.714em;vertical-align:0em"></span><span class="mord accent"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.714em"><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord mathnormal">x</span></span></span><span style="top:-3em"><span class="pstrut" style="height:3em"></span><span class="accent-body" style="left:-0.20772em"><span class="overlay" style="height:0.714em;width:0.471em"><svg width="0.471em" height="0.714em" style="width:0.471em" viewBox="0 0 471 714" preserveAspectRatio="xMinYMin"><path d="M377 20c0-5.333 1.833-10 5.5-14S391 0 397 0c4.667 0 8.667 1.667 12 5
3.333 2.667 6.667 9 10 19 6.667 24.667 20.333 43.667 41 57 7.333 4.667 11
10.667 11 18 0 6-1 10-3 12s-6.667 5-14 9c-28.667 14.667-53.667 35.667-75 63
-1.333 1.333-3.167 3.5-5.5 6.5s-4 4.833-5 5.5c-1 .667-2.5 1.333-4.5 2s-4.333 1
-7 1c-4.667 0-9.167-1.833-13.5-5.5S337 184 337 178c0-12.667 15.667-32.333 47-59
H213l-171-1c-8.667-6-13-12.333-13-19 0-4.667 4.333-11.333 13-20h359
c-16-25.333-24-45-24-59z"></path></svg></span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:2.40003em;vertical-align:-0.95003em"></span><span class="minner"><span class="mopen delimcenter" style="top:0em"><span class="delimsizing size3">[</span></span><span class="mord"><span class="mtable"><span class="col-align-c"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.45em"><span style="top:-3.61em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">1</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span><span style="top:-2.4099999999999997em"><span class="pstrut" style="height:3em"></span><span class="mord"><span class="mord"><span class="mord mathnormal">x</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.9500000000000004em"><span></span></span></span></span></span></span></span><span class="mclose delimcenter" style="top:0em"><span class="delimsizing size3">]</span></span></span></span></span></span></span></div><p>If we then multiply by a positive scaling value k, we can represent the linear transformation of scalar multiplication graphically like:</p><p><img alt="transform_demo" src="/APM-2020/assets/images/transform-70fc755dcbdff432a0324d8b28a05a25.png"></p><p>Linear transformations of course aren&#x27;t just scalars. They can consist of many different things. In fact, we have been using linear transformations even before neural networks! Differentiation is one such linear transformation. If you want more details on what constitutes a linear transformation, I would recommend looking at this wolfram alpha article: <a href="https://mathworld.wolfram.com/LinearTransformation.html" target="_blank" rel="noopener noreferrer">https://mathworld.wolfram.com/LinearTransformation.html</a>.</p><h3><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="supplemental-resources"></a>Supplemental Resources:<a aria-hidden="true" tabindex="-1" class="hash-link" href="#supplemental-resources" title="Direct link to heading">#</a></h3><p>Statquest video: <a href="https://www.youtube.com/watch?v=CqOfi41LfDw&amp;" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=CqOfi41LfDw&amp;</a></p><p>3blue1brown video series: <a href="https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=aircAruvnKk&amp;list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-22T00:55:23.000Z" class="docLastUpdatedAt_217_">10/21/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-07-sgd"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Stochastic Gradient Decent</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-09-mlp"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Multi-Layered Perceptron Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#summary-of-lecture" class="table-of-contents__link">Summary of Lecture</a><ul><li><a href="#analogy-of-a-neuron" class="table-of-contents__link">Analogy of a Neuron:</a></li><li><a href="#history-of-adaptive-linear-elements-qianhui" class="table-of-contents__link">History of Adaptive Linear Elements: Qianhui</a></li><li><a href="#neural-networks" class="table-of-contents__link">Neural Networks</a></li><li><a href="#linear-transformations" class="table-of-contents__link">Linear Transformations</a></li><li><a href="#supplemental-resources" class="table-of-contents__link">Supplemental Resources:</a></li></ul></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2cffed3b.js"></script>
<script src="/APM-2020/runtime~main.1402fe68.js"></script>
<script src="/APM-2020/main.2d013a91.js"></script>
<script src="/APM-2020/1.1bd411a9.js"></script>
<script src="/APM-2020/2.a2e25927.js"></script>
<script src="/APM-2020/32.9618ee48.js"></script>
<script src="/APM-2020/f976f453.84bb295e.js"></script>
<script src="/APM-2020/17896441.32dbdef4.js"></script>
<script src="/APM-2020/f1296005.13349ef3.js"></script>
</body>
</html>