<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Principal Component Analysis (PCA) | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Principal Component Analysis (PCA) | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Katelyn Vincent, Yiqun Tian, Yue Tian. (PDF)"><meta data-react-helmet="true" property="og:description" content="Authors: Katelyn Vincent, Yiqun Tian, Yue Tian. (PDF)"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-14-pca"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-14-pca"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.2f015344.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.32f049b1.js" as="script">
<link rel="preload" href="/APM-2020/main.8b7f41c7.js" as="script">
<link rel="preload" href="/APM-2020/1.e4c79b7e.js" as="script">
<link rel="preload" href="/APM-2020/2.9a4f08f3.js" as="script">
<link rel="preload" href="/APM-2020/37.aad1e583.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.d8642ff2.js" as="script">
<link rel="preload" href="/APM-2020/17896441.5ece2e3d.js" as="script">
<link rel="preload" href="/APM-2020/4d02b85e.cffba1ac.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-03-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-05-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-06-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-07-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-12-reducing-features">Reducing the Number of (Derived) Attributes/Features.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-13-dd-docs">Data-Driven Documents</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-14-pca">Principal Components Analysis</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-01-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-02-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-03-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-04-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-05-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-06-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-07-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-08-nn">Neural Networks</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-09-mlp">Multi-Layered Perceptron</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-10-dp">Data Preprocessing</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-11-outliers">Handling Outliers</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-12-reducing-features">All About Features</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-13-dd-docs">Interactive Online Visualization</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-14-pca">Principal Component Analysis (PCA)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-15-dt">Classification - Decision Trees and Bayes Decision Theory</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Principal Component Analysis (PCA)</h1></header><div class="markdown"><p>Authors: Katelyn Vincent, Yiqun Tian, Yue Tian. (<a target="_blank" href="/APM-2020/assets/files/b-14-pca-8a237557046ada44e5592de0fe9e36b7.pdf">PDF</a>)</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="what-is-pca"></a>What is PCA?<a aria-hidden="true" tabindex="-1" class="hash-link" href="#what-is-pca" title="Direct link to heading">#</a></h2><p>In this lecture, we discussed feature extraction methods, focusing primarily on Principal Component Analysis (PCA).  PCA is the simplest way of trying to project data into a low dimensional space.  The goal is to reduce dimensionality while keeping as much variance (spread) as you can from the original data.  Think about drawing a line and pressing on either side of that line until you squash all of the data points onto the line.  This line is our principal component (PC), and we are â€˜projectingâ€™ all of the data points onto it.  There are two ways of thinking about how to find the best line: we want to find the line that 1) maximizes the spread of data and 2) minimizes the sum of squared residuals.  </p><p><img src="https://blog.umetrics.com/hubfs/Blog%20images/figure%203.2%20PCA%20blog%2025.png"></p><p>We can have more than one PC, and each additional PC is orthogonal (at a right angle) to all of the other PCs.  Sometimes what happens is that PC1 will tell you what the overall spread of the data is, and additional PCs (ex. PC2, PC3, PC4) will tell you how subpopulations differ from one another.  Each PC has an <strong>eigenvalue</strong> - a measure of how much variance is retained.  PC1 will have the highest eigenvalue (eg 7.5), PC2 will have the second highest (eg. 2) and so on.  In this example, we can say that the first two principal components capture 95% of the variance in the data (7.5 + 2).  Typically, youâ€™ll want to pick enough principal components to cover 90-95% of the variance.  If the first few eigenvalues are much higher than the others, PCA can capture the data using a much lower dimensional space.  If all of the eigenvalues are roughly the same, it means the data falls the same way in every direction and thereâ€™s no point in doing PCA. </p><p><img src="https://i.imgur.com/z06XIbn.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="when-are-the-pros-and-cons-of-pca-and-when-should-you-use-it"></a>When are the pros and cons of PCA, and when should you use it?<a aria-hidden="true" tabindex="-1" class="hash-link" href="#when-are-the-pros-and-cons-of-pca-and-when-should-you-use-it" title="Direct link to heading">#</a></h2><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="pros-removes-correlated-features-reduces-overfitting-and-improves-performance"></a>Pros: Removes correlated features, reduces overfitting, and improves performance<a aria-hidden="true" tabindex="-1" class="hash-link" href="#pros-removes-correlated-features-reduces-overfitting-and-improves-performance" title="Direct link to heading">#</a></h4><p>One of the most obvious benefits of PCA is that it reduces dimensionality while retaining most of the information and variance in our original features.  PCA is helpful when you need to reduce the number of features for modeling, but it&#x27;s not clear that there are individual variables you should remove.  It is also helpful if you need to be sure that your features are independent of one another, since each of the principal components will be independent of one another. Because PCA reduces the number of features, it also helps to speed up training time.  </p><h4><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="cons-variables-are-less-interpretable-requires-standardization-and-possibility-of-information-loss"></a>Cons: Variables are less interpretable, requires standardization, and possibility of information loss<a aria-hidden="true" tabindex="-1" class="hash-link" href="#cons-variables-are-less-interpretable-requires-standardization-and-possibility-of-information-loss" title="Direct link to heading">#</a></h4><p>Because PCA combines information from multiple original features, the principal components are less interpretable than the original features.  PCA is affected by scale, so it is also important to scale features before applying PCA, as well as convert categorical variables to numeric.  Additionally, selecting too few principal components can result in information loss. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="what-is-the-difference-between-pca-and-linear-regression"></a>What is the difference between PCA and Linear Regression?<a aria-hidden="true" tabindex="-1" class="hash-link" href="#what-is-the-difference-between-pca-and-linear-regression" title="Direct link to heading">#</a></h2><p>PCA is not linear regression. In fact, PCA and linear regression use totally different algorithms. For linear regression, it is trying to predict the value of Y given some info features of X and fit a straight line as to minimize the square error between the point and this straight line. However, for PCA there is no special variable Y that we are trying to predict. PCA minimizes the shortest orthogonal distance. </p><p><img src="https://i.imgur.com/nkE4Uxx.png">
<img src="https://i.imgur.com/YNvHmsR.png"></p><p>On the left graph, we can see linear regression calculates the square error as the vertical distance between true value and predicted value. On the right graph, PCA calculates the projected error as the shortest distance between true value and projected line. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="alternative-for-classification---linear-discriminant-analysis-lda"></a>Alternative for Classification - Linear Discriminant Analysis (LDA)<a aria-hidden="true" tabindex="-1" class="hash-link" href="#alternative-for-classification---linear-discriminant-analysis-lda" title="Direct link to heading">#</a></h2><p>PCA does not use class information, so if you&#x27;re trying to do classification then a better alternative might be Linear Discriminant Analysis (LDA).  As the name implies, the technique is also linear but uses the class levels (unlike PCA).  LDA can either be used for classification problems or as a dimensionality reduction technique in preprocessing.  While Logistic regression is a classification algorithm traditionally limited to only two-class classification problems, if you have more than two classes then Linear Discriminant Analysis is the preferred linear classification technique.  LDA essentially plots your features, then creates a new axis that 1) maximizes the distance between means of the two classes and 2) minimizes the variation within each class.  In simple terms, this newly generated axis increases the separation between the data points of the two classes.</p><p><img src="https://images2.programmersought.com/460/b7/b7854fa863ed93f55db4cb389ba12b44.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="manifold-destiny"></a>Manifold Destiny<a aria-hidden="true" tabindex="-1" class="hash-link" href="#manifold-destiny" title="Direct link to heading">#</a></h2><p>While this discussion has mostly focused on two-dimensional examples, there are many datasets where you can do a much better job of capturing the data if the projection space is curved (instead of flat).  These surfaces are caled <strong>manifolds</strong>.  In simple terms, an n-dimensional manifold is a space that locally looks like n-dimensional Euclidean space. You can think of it as putting a lot of n-dimensional shapes together to build a space. A simple example can be a flat map of the Earth. A map is a two-dimensional representation of the three-dimensional sphere, the Earth. Imagine the Earth has XYZ coordinates: locally, the z coordinate is barely changing, so the two-dimensional (XY coordinates) approximation is a good approximation about where a certain point is on the map.  One of the most populat techniques to do nonlinear transformation mappings and find these manifolds is <strong>t-sne</strong>.</p><p><img src="https://www.researchgate.net/profile/Kilian_Weinberger/publication/201841023/figure/fig1/AS:276492534730763@1442932365687/The-problem-of-manifold-learning-illustrated-for-N-800-data-points-sampled-from-a.png"></p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="references"></a>References<a aria-hidden="true" tabindex="-1" class="hash-link" href="#references" title="Direct link to heading">#</a></h2><p><a href="https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used" target="_blank" rel="noopener noreferrer">https://blog.umetrics.com/what-is-principal-component-analysis-pca-and-how-it-is-used</a></p><p><a href="https://www.i2tutorials.com/what-are-the-pros-and-cons-of-the-pca/" target="_blank" rel="noopener noreferrer">https://www.i2tutorials.com/what-are-the-pros-and-cons-of-the-pca/</a></p><p><a href="https://sebastianraschka.com/Articles/2014_python_lda.html" target="_blank" rel="noopener noreferrer">https://sebastianraschka.com/Articles/2014_python_lda.html</a></p><p><a href="https://www.programmersought.com/article/33174132390/" target="_blank" rel="noopener noreferrer">https://www.programmersought.com/article/33174132390/</a></p><p><a href="https://www.researchgate.net/figure/The-problem-of-manifold-learning-illustrated-for-N-800-data-points-sampled-from-a_fig1_201841023" target="_blank" rel="noopener noreferrer">https://www.researchgate.net/figure/The-problem-of-manifold-learning-illustrated-for-N-800-data-points-sampled-from-a_fig1_201841023</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-31T08:10:29.000Z" class="docLastUpdatedAt_217_">10/31/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-13-dd-docs"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Interactive Online Visualization</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-15-dt"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Classification - Decision Trees and Bayes Decision Theory Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#what-is-pca" class="table-of-contents__link">What is PCA?</a></li><li><a href="#when-are-the-pros-and-cons-of-pca-and-when-should-you-use-it" class="table-of-contents__link">When are the pros and cons of PCA, and when should you use it?</a></li><li><a href="#what-is-the-difference-between-pca-and-linear-regression" class="table-of-contents__link">What is the difference between PCA and Linear Regression?</a></li><li><a href="#alternative-for-classification---linear-discriminant-analysis-lda" class="table-of-contents__link">Alternative for Classification - Linear Discriminant Analysis (LDA)</a></li><li><a href="#manifold-destiny" class="table-of-contents__link">Manifold Destiny</a></li><li><a href="#references" class="table-of-contents__link">References</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.2f015344.js"></script>
<script src="/APM-2020/runtime~main.32f049b1.js"></script>
<script src="/APM-2020/main.8b7f41c7.js"></script>
<script src="/APM-2020/1.e4c79b7e.js"></script>
<script src="/APM-2020/2.9a4f08f3.js"></script>
<script src="/APM-2020/37.aad1e583.js"></script>
<script src="/APM-2020/f976f453.d8642ff2.js"></script>
<script src="/APM-2020/17896441.5ece2e3d.js"></script>
<script src="/APM-2020/4d02b85e.cffba1ac.js"></script>
</body>
</html>