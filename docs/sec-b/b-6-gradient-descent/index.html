<!doctype html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="generator" content="Docusaurus v2.0.0-alpha.63">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous"><title data-react-helmet="true">Beyond Linear Regression and Gradient Descent | MIS 382N: Advaned Predictive Modelling</title><meta data-react-helmet="true" name="docsearch:version" content="current"><meta data-react-helmet="true" name="twitter:card" content="summary_large_image"><meta data-react-helmet="true" property="og:title" content="Beyond Linear Regression and Gradient Descent | MIS 382N: Advaned Predictive Modelling"><meta data-react-helmet="true" name="description" content="Authors: Shruti Kapur, Bhavna Kaparaju, Ram Kapistalam"><meta data-react-helmet="true" property="og:description" content="Authors: Shruti Kapur, Bhavna Kaparaju, Ram Kapistalam"><meta data-react-helmet="true" property="og:url" content="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-6-gradient-descent"><link data-react-helmet="true" rel="shortcut icon" href="/APM-2020/img/favicon.ico"><link data-react-helmet="true" rel="canonical" href="https://ideal-ut.github.io/APM-2020/docs/sec-b/b-6-gradient-descent"><link rel="stylesheet" href="/APM-2020/styles.12cdeefe.css">
<link rel="preload" href="/APM-2020/styles.38b57550.js" as="script">
<link rel="preload" href="/APM-2020/runtime~main.3ed142f5.js" as="script">
<link rel="preload" href="/APM-2020/main.4497b5ac.js" as="script">
<link rel="preload" href="/APM-2020/1.21d32170.js" as="script">
<link rel="preload" href="/APM-2020/2.2e79fbbb.js" as="script">
<link rel="preload" href="/APM-2020/24.86a84938.js" as="script">
<link rel="preload" href="/APM-2020/f976f453.6f9befb5.js" as="script">
<link rel="preload" href="/APM-2020/17896441.ac627176.js" as="script">
<link rel="preload" href="/APM-2020/a2723199.3c046eee.js" as="script">
</head>
<body>
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}()</script><div id="__docusaurus">
<nav class="navbar navbar--light navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><div aria-label="Navigation bar toggle" class="navbar__toggle" role="button" tabindex="0"><svg xmlns="http://www.w3.org/2000/svg" width="30" height="30" viewBox="0 0 30 30" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></div><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/APM-2020/docs/">Home</a></div><div class="navbar__items navbar__items--right"><div class="react-toggle react-toggle--disabled displayOnlyInLargeViewport_2aTZ"><div class="react-toggle-track"><div class="react-toggle-track-check"><span class="toggle_BsTx">ðŸŒœ</span></div><div class="react-toggle-track-x"><span class="toggle_BsTx">ðŸŒž</span></div></div><div class="react-toggle-thumb"></div><input type="checkbox" disabled="" aria-label="Dark mode toggle" class="react-toggle-screenreader-only"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div><div class="navbar-sidebar"><div class="navbar-sidebar__brand"><a class="navbar__brand" href="/APM-2020/"><strong class="navbar__title"></strong></a></div><div class="navbar-sidebar__items"><div class="menu"><ul class="menu__list"><li class="menu__list-item"><a aria-current="page" class="menu__link navbar__link--active" href="/APM-2020/docs/">Home</a></li></ul></div></div></div></nav><div class="main-wrapper"><div class="docPage_2gpo"><div class="docSidebarContainer_3_JD" role="complementary"><div class="sidebar_2urC"><div class="menu menu--responsive menu_5FrY"><button aria-label="Open Menu" aria-haspopup="true" class="button button--secondary button--sm menu__button" type="button"><svg aria-label="Menu" class="sidebarMenuIcon_Dm3K" xmlns="http://www.w3.org/2000/svg" height="24" width="24" viewBox="0 0 32 32" role="img" focusable="false"><title>Menu</title><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" href="/APM-2020/docs/">Info</a></li><li class="menu__list-item menu__list-item--collapsed"><a class="menu__link menu__link--sublist" href="#!">Section A</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-3-parametric-models">Parametric Models</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-5-bias-variance">Bias and Variance Dilemma &amp; Shrinkage Methods</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-6-gradient-descent">Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-7-sgd">Stochastic Gradient Descent (SGD) &amp; Neural Network Intro.</a></li><li class="menu__list-item"><a class="menu__link" tabindex="-1" href="/APM-2020/docs/sec-a/a-8-nn">Neural Networks</a></li></ul></li><li class="menu__list-item"><a class="menu__link menu__link--sublist menu__link--active" href="#!">Section B</a><ul class="menu__list"><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-1-gaussian-dist">The Gaussian Distribution</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-2-mlr">Multiple Linear Regression</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-3-parametric-models">Function approximation (Regression)</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-4-regularization">Regularization</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-5-bias-variance">Bias-Variance Dilemma</a></li><li class="menu__list-item"><a aria-current="page" class="menu__link menu__link--active active" tabindex="0" href="/APM-2020/docs/sec-b/b-6-gradient-descent">Beyond Linear Regression and Gradient Descent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-7-sgd">Stochastic Gradient Decent</a></li><li class="menu__list-item"><a class="menu__link" tabindex="0" href="/APM-2020/docs/sec-b/b-8-nn">Neural Networks</a></li></ul></li></ul></div></div></div><main class="docMainContainer_3EyW"><div class="container padding-vert--lg docItemWrapper_1EkI"><div class="row"><div class="col docItemCol_2ASc"><div class="docItemContainer_3QWW"><article><div><span class="badge badge--secondary">Version: Next</span></div><header><h1 class="docTitle_1Lrw">Beyond Linear Regression and Gradient Descent</h1></header><div class="markdown"><p>Authors: Shruti Kapur, Bhavna Kaparaju, Ram Kapistalam</p><p>To start the class, we touched on &quot;Almost&quot; Linear models (such as Piecewise Linear) and nonlinear models. We then discussed the nature of Universal Approximators and some specific examples of this concept such as Neural Nets. Finally, we looked at three methods to find the values of a function&#x27;s parameters (coefficients) that minimize a cost function as far as possible. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="universal-approximators"></a>Universal Approximators<a aria-hidden="true" tabindex="-1" class="hash-link" href="#universal-approximators" title="Direct link to heading">#</a></h2><p>The class first explained the idea of linear regression forms as a subset of universal approximators. Universal approximators are functions or theorems that can produce an output which is an accurate approximation to any desired degree of accuracy of functions in certain familiar spaces of functions. MLPs, polynomial functions and certain neural networks are good examples of such approximators.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="minimizing-cost"></a>Minimizing Cost<a aria-hidden="true" tabindex="-1" class="hash-link" href="#minimizing-cost" title="Direct link to heading">#</a></h2><p>The class then further built on the concept of minimizing the cost E(a) = SSE of MSE. We can think of different ways of finding the weights. The first involves linear regression, which gives us a direct solutions with optimal weights (w*). </p><p>The second method is the very interesting Newton Raphson method of finding the roots. This method essentially uses the idea that a continuous and differentiable function can be approximated by a straight line tangent to it. This method gives a one step guess to the optimum weights, when we consider that E(w) is quadratic in w and w*  is the optimum solution. </p><p>Though the class did not delve much into this method, we studied a bit and found two interesting facts associated to it. The first is that the function may not always converge (counter-intuitively). Its convergence theory is for &quot;local&quot; convergence which means we should start close to the root, where &quot;close&quot; is relative to the function we are dealing with. Far away from the root we can have highly nontrivial dynamics. The second interesting fact is that Newton&#x27;s method will fail in cases where the derivative is zero. When the derivative is close to zero, the tangent line is nearly horizontal and hence may overshoot the desired root (numerical difficulties). Solution: Try another initial point. </p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="gradient-descent"></a>Gradient Descent<a aria-hidden="true" tabindex="-1" class="hash-link" href="#gradient-descent" title="Direct link to heading">#</a></h2><p>The third method and the core focus of the class was the idea of a Gradient Descent. The concept is that we initially guess the optimum weight, call it w0 and iteratively update w by going â€˜downâ€™ the gradient till we reach the optimum weight, w*. This method works even for nonlinear models.</p><p>For instance, if the cost function E(w) is quadratic in w, weights can be obtained by incrementally moving down the cost surface in weight space.</p><p><a href="https://i.imgur.com/MlW0MMZ.png" target="_blank" rel="noopener noreferrer"><img src="https://i.imgur.com/MlW0MMZ.png"></a></p><p>The learning rate is important â€“ too low would cause slow convergence and too high would cause instability. There are variations in the learning rates. </p><ul><li>For instance, an adaptive learning rate involves Choosing a decrease constant d that shrinks the learning rate over time: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î·</mi><mo stretchy="false">(</mo><mi>t</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo><mo>:</mo><mo>=</mo><mi>Î·</mi><mo stretchy="false">(</mo><mi>t</mi><mo stretchy="false">)</mo><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><mi>t</mi><mo>Ã—</mo><mi>d</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\eta(t+1):=\eta(t)/(1+t\times d)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">Î·</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord">1</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.03588em">Î·</span><span class="mopen">(</span><span class="mord mathnormal">t</span><span class="mclose">)</span><span class="mord">/</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:0.69841em;vertical-align:-0.08333em"></span><span class="mord mathnormal">t</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">Ã—</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal">d</span><span class="mclose">)</span></span></span></span></span></li><li>Another method is momentum learning by adding a factor of the previous gradient to the weight update for faster updates: <span class="math math-inline"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â–³</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo>:</mo><mo>=</mo><mi>Î·</mi><mo>â–½</mo><mi>J</mi><mo stretchy="false">(</mo><msub><mi>w</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>+</mo><mi>Î±</mi><mo>â–³</mo><msub><mi>w</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\bigtriangleup w_{t+1}:=\eta\bigtriangledown J(w_{t+1})+\alpha\bigtriangleup w_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.902771em;vertical-align:-0.208331em"></span><span class="mord">â–³</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em"></span><span class="mrel">:</span></span><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.03588em">Î·</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">â–½</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em"></span><span class="mord mathnormal" style="margin-right:0.09618em">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em"></span><span class="mord mathnormal" style="margin-right:0.0037em">Î±</span><span class="mspace" style="margin-right:0.2222222222222222em"></span><span class="mbin">â–³</span><span class="mspace" style="margin-right:0.2222222222222222em"></span></span><span class="base"><span class="strut" style="height:0.58056em;vertical-align:-0.15em"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02691em">w</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em"><span style="top:-2.5500000000000003em;margin-left:-0.02691em;margin-right:0.05em"><span class="pstrut" style="height:2.7em"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">â€‹</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em"><span></span></span></span></span></span></span></span></span></span></span></li></ul><p>There are a variety of gradient descent optimization algorithms in the market today, such as Momentum, Nesterov, Adagrad, Adadelta, RMSprop, Adam and Nadam. Adam is the most popular one â€“ ADAM stands for adaptive Moment Estimation. ADAM computes adaptive learning rates for each parameter. It also keeps an exponentially decaying average of past gradients mt, similar to Momentum. </p><p>The class also briefly mentioned the contrast between True Gradient Descent and Stochastic Gradient Descent. One difference mentioned is that True Gradient Descent uses a Batch Algorithm, meaning that it involves all of the training data. The Medium article in &quot;Additional Resources&quot; explains why that is in-depth, but here are a couple reasons that a batch algorithm is used:</p><ul><li>There are less oscillations and noisy steps taken towards the global minima of the loss function because you are updating the parameters by taking the average of all training samples rather than just one sample.
</li><li>It allows for a more stable gradient descent convergence and stable error gradient than Stochastic Gradient Descent
</li></ul><p>Still, there are some disadvantages mentioned related to True Gradient Descent. The most common issues relate to the processing time needed for this method since you are processing the entire training sample. Another common problem with True Gradient Descent is that sometimes a stable error gradient can lead to a local minima rather than the optimal global minima.</p><h2><a aria-hidden="true" tabindex="-1" class="anchor enhancedAnchor_2cZh" id="additional-resources"></a>Additional Resources<a aria-hidden="true" tabindex="-1" class="hash-link" href="#additional-resources" title="Direct link to heading">#</a></h2><p>Newton-Raphson Method: <a href="https://towardsdatascience.com/newtons-method-the-visual-intuition-12a346f4d89" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/newtons-method-the-visual-intuition-12a346f4d89</a></p><p>Andrew Ng on Gradient Descent: <a href="https://www.youtube.com/watch?v=rIVLE3condE" target="_blank" rel="noopener noreferrer">https://www.youtube.com/watch?v=rIVLE3condE</a></p><p>Article on Gradient Descent: <a href="https://towardsdatascience.com/gradient-descent-algorithms-and-adaptive-learning-rate-adjustment-methods-79c701b086be" target="_blank" rel="noopener noreferrer">https://towardsdatascience.com/gradient-descent-algorithms-and-adaptive-learning-rate-adjustment-methods-79c701b086be</a></p><p>Variations of Gradient Descent: <a href="https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1" target="_blank" rel="noopener noreferrer">https://medium.com/@divakar_239/stochastic-vs-batch-gradient-descent-8820568eada1</a></p></div></article><div class="margin-vert--xl"><div class="row"><div class="col"></div><div class="col text--right"><em><small>Last updated on <time datetime="2020-10-07T00:00:55.000Z" class="docLastUpdatedAt_217_">10/6/2020</time></small></em></div></div></div><div class="margin-vert--lg"><nav class="pagination-nav" aria-label="Blog list page navigation"><div class="pagination-nav__item"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-5-bias-variance"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Â« Bias-Variance Dilemma</div></a></div><div class="pagination-nav__item pagination-nav__item--next"><a class="pagination-nav__link" href="/APM-2020/docs/sec-b/b-7-sgd"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">Stochastic Gradient Decent Â»</div></a></div></nav></div></div></div><div class="col col--3"><div class="tableOfContents_3SO_"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#universal-approximators" class="table-of-contents__link">Universal Approximators</a></li><li><a href="#minimizing-cost" class="table-of-contents__link">Minimizing Cost</a></li><li><a href="#gradient-descent" class="table-of-contents__link">Gradient Descent</a></li><li><a href="#additional-resources" class="table-of-contents__link">Additional Resources</a></li></ul></div></div></div></div></main></div></div><footer class="footer footer--dark"><div class="container"><div class="text--center"><div>Copyright Â© 2020 Authors. Built with Docusaurus.</div></div></div></footer></div>
<script src="/APM-2020/styles.38b57550.js"></script>
<script src="/APM-2020/runtime~main.3ed142f5.js"></script>
<script src="/APM-2020/main.4497b5ac.js"></script>
<script src="/APM-2020/1.21d32170.js"></script>
<script src="/APM-2020/2.2e79fbbb.js"></script>
<script src="/APM-2020/24.86a84938.js"></script>
<script src="/APM-2020/f976f453.6f9befb5.js"></script>
<script src="/APM-2020/17896441.ac627176.js"></script>
<script src="/APM-2020/a2723199.3c046eee.js"></script>
</body>
</html>